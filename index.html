<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>AICL Lectures 2025</title>

    <link rel="stylesheet" href="css/custom.css" />
    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" />
    <link rel="stylesheet" href="plugin/highlight/monokai.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.5/gsap.min.js"></script>
    <style>
      .inline-logo {
        height: 1em;
        vertical-align: middle;
        margin-left: 0.3em;
        position: relative;
        top: -1px;
      }
      .badge {
        display: inline-block;
        background: #444;
        color: #fff;
        border-radius: 0.4em;
        padding: 0.1em 0.5em;
        font-size: 0.9em;
        font-family: monospace;
        vertical-align: middle;
      }
      .checkpoint-animate {
        background: linear-gradient(90deg, #ffe259, #ffa751, #ffe259);
        background-size: 200% auto;
        color: #222;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: checkpoint-glow 2s linear infinite;
        font-weight: bold;
        text-shadow: 0 0 10px #ffe25988, 0 0 20px #ffa75144;
      }
      @keyframes checkpoint-glow {
        0% {
          background-position: 0% 50%;
        }
        100% {
          background-position: 100% 50%;
        }
      }

      #countdown-timer {
        font-size: 5em;
        background: linear-gradient(90deg, #00c3ff, #ffff1c, #ff5e62, #00c3ff);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin: 40px auto 0 auto;
        width: fit-content;
        animation: gradient-move 3s ease-in-out infinite,
          text-shadow-pulse 2s ease-in-out infinite;
        letter-spacing: 0.1em;
      }
      @keyframes gradient-move {
        0% {
          background-position: 0% 50%;
        }
        50% {
          background-position: 100% 50%;
        }
        100% {
          background-position: 0% 50%;
        }
      }
      @keyframes pulse-timer {
        0% {
          box-shadow: 0 0 40px #00c3ff55, 0 0 40px #ff5e6255;
        }
        50% {
          box-shadow: 0 0 80px #00c3ff99, 0 0 80px #ff5e6299;
        }
        100% {
          box-shadow: 0 0 40px #00c3ff55, 0 0 40px #ff5e6255;
        }
      }
      .timer-blink {
        animation: blink-timer 0.7s steps(1) infinite;
      }
      @keyframes blink-timer {
        0%,
        100% {
          color: #fff;
          background: #e74c3c;
        }
        50% {
          color: #e74c3c;
          background: #fff;
        }
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2 class="gradient-title">
            Building a RAG System with LangChain and <br />ICD-11 Data
          </h2>
          <img src="imgs/qr-code.png" />
          <p>
            <a href="https://unimib-whattadata.github.io/RAGLecture/"
              >https://unimib-whattadata.github.io/RAGLecture/</a
            >
          </p>
        </section>

        <section>
          <img class="r-stretch" src="imgs/AILC_Logo.png" />
          <h2 class="gradient-title">
            Building a RAG System with LangChain and <br />ICD-11 Data
          </h2>
          <p>
            Retrieval Augmented Generation, Large Language Models and Knowledge
            Bases
          </p>

          <aside class="notes">
            <p>
              <strong>Good morning, everyone.</strong> Welcome to our lab,
              <strong
                >'Building a Retrieval Augmented Generation System with
                LangChain and ICD-11 Data.'</strong
              >
              Over the next two hours, we'll explore
              <strong>why RAG matters</strong> and set up the environment
              together.
            </p>
          </aside>
        </section>

        <!-- ===== SPEAKER ===== -->
        <section>
          <div style="text-align: center">
            <img
              src="imgs/speaker/mc-coloured.png"
              alt="Marco Cremaschi"
              style="max-width: 400px"
            />
            <div style="margin-top: 10px">
              <a href="https://www.linkedin.com/in/marco-cremaschi/"
                >Marco Cremaschi</a
              >
            </div>
          </div>

          <aside class="notes">
            <p>
              My name is <strong>Marco Cremaschi</strong>. I'm a
              <strong>researcher</strong> at the University of Milan-Bicocca and
              a <strong>lecturer</strong> in a database course for the master's
              degree in <strong>Computational Linguistics</strong>.
            </p>
          </aside>
        </section>

        <!-- ===== TUTORS ===== -->
        <section>
          <h2>Tutors</h2>
          <div
            style="
              display: flex;
              justify-content: center;
              align-items: flex-start;
              gap: 60px;
              margin-top: 40px;
            "
          >
            <div style="text-align: center">
              <img
                src="imgs/speaker/dc-coloured.png"
                alt="David Chieregato"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">
                <a href=""
                  ><a
                    href="https://www.linkedin.com/in/david-chieregato-8110b6b4/"
                    >David Chieregato</a
                  ></a
                >
              </div>
            </div>
            <div style="text-align: center">
              <img
                src="imgs/speaker/fd-coloured.png"
                alt="Fabio D'Adda"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">
                <a href="https://www.linkedin.com/in/fabio-d-adda5/"
                  >Fabio D'Adda</a
                >
              </div>
            </div>
            <div style="text-align: center">
              <img
                src="imgs/speaker/dd-coloured.png"
                alt="Davide Ditolve"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">
                <a href="https://www.linkedin.com/in/davide-ditolve/"
                  >Davide Ditolve</a
                >
              </div>
            </div>
          </div>
          <aside class="notes">
            <p>
              For this lab, we also have the help of <strong>David</strong>,
              <strong>Fabio</strong>, and <strong>Davide</strong>. We'll walk
              around during the <strong>coding checkpoints</strong>, so if you
              need any help, just raise your hand.
            </p>
          </aside>
        </section>

        <!-- ===== LAB AGENDA ===== -->
        <section>
          <h2 class="title-animate">
            <span class="maya-blue"></span>LAB Agenda
          </h2>
        </section>

        <!-- ===== LAB AGENDA LIST ===== -->
        <section>
          <ul class="gradient-list">
            <li>
              <span class="tekhelet">Part 1</span>: The Fundamentals (RAG &
              ICD-11)
            </li>
            <li>
              <span class="medium-slate-blue">Part 2</span>: Prerequisites,
              Setup, Data Loading & Processing
            </li>
            <li>
              <span class="selective-yellow">Part 3</span>: Embeddings & Vector
              Stores
            </li>
            <li>
              <span class="tangerine">Part 4</span>: Building the RAG Chain
            </li>
            <li>
              <span class="persimmon">Part 5</span>: Querying the RAG System
            </li>
            <li>>Conclusion & Q&A</li>
          </ul>

          <aside class="notes">
            Here's an overview of today's lab. We've divided the session into
            five parts, each one directly aligned with a code checkpoint:

            <ul>
              <li>
                In Part 1, we'll cover the conceptual foundations, looking at
                what RAG is and how ICD-11 serves as our structured knowledge
                base.
              </li>

              <li>
                In Part 2, we'll move to the technical setup, including
                prerequisites, Colab configuration, and data loading.
              </li>

              <li>
                Part 3 introduces embeddings and vector stores—this is how we
                make our text searchable and retrievable.
              </li>

              <li>
                In Part 4, we'll see how to build the actual RAG chain,
                connecting retrieval and generation components using LangChain.
              </li>

              <li>
                And in Part 5, we'll query the system interactively and observe
                how it returns grounded responses.
              </li>
            </ul>

            We'll end with a wrap-up and Q&A session.
          </aside>
        </section>

        <!-- ===== PART 1: THE FUNDAMENTALS ===== -->
        <section id="part-1">
          <h2 class="title-animate">
            <span class="tekhelet">Part 1</span>: The Fundamentals (RAG &
            ICD-11)
          </h2>

          <aside class="notes">
            <p>
              For the first part, we'll examine the
              <strong>conceptual side</strong>, covering what
              <strong>RAG</strong> is and why <strong>ICD-11</strong> makes a
              suitable demonstration dataset.
            </p>
          </aside>
        </section>

        <!-- ===== WHAT IS RAG ===== -->
        <section>
          <h2>What is RAG?</h2>
          <p>
            <span class="gradient-title"
              >Retrieval-Augmented Generation (RAG)</span
            >
            is an AI framework for improving the quality of LLM-generated
            responses by grounding the model on external sources of knowledge
          </p>
          <p>
            It combines a retriever (to find relevant information) with a
            generator (an LLM to craft an answer)
          </p>

          <aside class="notes">
            <p>
              <strong>Retrieval-Augmented Generation</strong> couples a
              <strong>retriever</strong> with a <strong>generator</strong> so
              the LLM is grounded in real data. Think of it as '<strong
                >search then answer</strong
              >' rather than '<strong>just answer</strong>'.
            </p>
          </aside>
        </section>

        <!-- ===== WHAT IS ICD-11 ===== -->
        <section>
          <h2>
            The Knowledge Base: <span class="gradient-title">ICD-11</span>
          </h2>

          <p>
            The International Classification of Diseases, 11th Revision (ICD-11)
            is the global standard for diagnostic health information
          </p>
          <p>
            The ICD-11 now catalogues roughly
            <span id="counting-number" class="highlight-number">0</span>
            distinct clinical concepts, giving health professionals an
            unprecedented breadth of diagnostic detail
          </p>
          <p>
            You can learn more at the
            <a href="https://icd.who.int/en" target="_blank"
              >official WHO site</a
            >
          </p>

          <aside class="notes">
            <p>
              Before we write a single line of code, let's examine what we're
              indexing. <strong>ICD-11</strong> is the
              <strong>World Health Organization's global standard</strong> for
              diagnostic codes. It went live in January 2022 and now contains
              approximately <strong>17,000 entities</strong> and approximately
              <strong>120,000 index terms</strong>.<br /><strong
                >Does someone knows ICD-11?</strong
              >
            </p>
          </aside>
        </section>

        <!-- ===== ICD-11 DATA USAGE ===== -->
        <section>
          <h2>
            The Knowledge Base: <span class="gradient-title">ICD-11</span>
          </h2>
          <p>
            Our RAG system will use this data from Chapter 6 to obtain
            suggestions for diagnoses
          </p>

          <aside class="notes">
            <p>
              Our work focuses specifically on <strong>Chapter 6</strong>, which
              addresses
              <strong
                >mental, behavioural, and neurodevelopmental disorders</strong
              >. This chapter includes all the conditions typically encountered
              in <strong>psychiatric and psychological practice</strong>, and
              forms the foundation of the dataset used to train our model.
            </p>
          </aside>
        </section>

        <!-- ===== ICD-11 BROWSER ===== -->
        <section>
          <p>
            <img src="imgs/icd-11-screen.png" />
            <a href="https://icd.who.int/browse/2025-01/mms/en#334423054"
              >ICD-11 Browser</a
            >
          </p>
          <aside class="notes">
            <p>
              Each ICD-11 entity comes with a unique
              <strong>alphanumeric code</strong> (e.g., 6B60) that serves as a
              identifier across systems. It includes a
              <strong>preferred title</strong> that reflects the official
              diagnostic label, as well as a set of <strong>synonyms</strong> to
              capture alternate or commonly used terms. A
              <strong>concise definition</strong> provides a clear and
              consistent explanation of the condition, ensuring semantic
              interoperability across clinical and research contexts. Entities
              are also organised into a <strong>hierarchical structure</strong>,
              with <strong>parent-child relationships</strong> that define
              broader or narrower diagnostic categories. This makes it easier to
              model clinical reasoning and support structured queries. In
              addition, each entry lists <strong>explicit exclusions</strong>,
              which specify what the diagnosis should <em>not</em> be confused
              with.
            </p>
          </aside>
        </section>

        <!-- ===== PART 2: PROJECT SETUP ===== -->
        <section>
          <h2 class="title-animate">
            <span class="medium-slate-blue">Part 2</span>: Project Setup
          </h2>

          <aside class="notes">
            <p>
              Let's get our environment ready. We'll begin with the
              <strong>requirements</strong> and then proceed to the
              <strong>setup</strong>.
            </p>
          </aside>
        </section>

        <!-- ===== LOCAL ENVIRONMENT VS. GOOGLE COLAB ===== -->
        <section>
          <h2>Local Environment vs. Google Colab</h2>
          <p>You can run Python code in different places:</p>
          <div style="display: flex; gap: 20px">
            <div
              style="
                flex: 1;
                background: rgba(255, 255, 255, 0.1);
                padding: 15px;
                border-radius: 10px;
              "
            >
              <h4>Your PC</h4>
              <ul>
                <li>
                  <strong>Setup:</strong> You install Python and libraries
                  yourself
                </li>
                <li><strong>Control:</strong> You have full control</li>
                <li><strong>Offline:</strong> Works without internet</li>
              </ul>
            </div>
            <div
              style="
                flex: 1;
                background: rgba(255, 255, 255, 0.1);
                padding: 15px;
                border-radius: 10px;
              "
            >
              <h4>
                Google Colab
                <img
                  class="inline-logo"
                  src="imgs/logo/colab-logo.png"
                  alt="Colab Logo"
                />
              </h4>
              <ul>
                <li><strong>Setup:</strong> No setup needed!</li>
                <li>
                  <strong>Pre-installed:</strong> Many libraries are already
                  available
                </li>
                <li>
                  <strong>Hardware:</strong> Free access to powerful hardware
                </li>
              </ul>
            </div>
          </div>

          <aside class="notes">
            <p>
              For the lab you can use a jupter notebook that runs on yout pc o
              google colab. compares running Python code If you prefer to work
              on your PC, you need to install Python and any required libraries
              manually. On the other hand,
              <strong>Google Colab</strong> requires <strong>no setup</strong>,
              comes with many libraries <strong>pre-installed</strong>, and, as
              we will descrive, provides
              <strong>free access to powerful hardware</strong> like GPUs.
            </p>
          </aside>
        </section>

        <!-- ===== INSTALLING LIBRARIES ===== -->
        <section>
          <h2>Local Environment vs. Google Colab</h2>
          <p>For this LAB, both work perfectly fine!</p>
          <p>
            The code will be discussed on Google Colab
            <img
              class="inline-logo"
              src="imgs/logo/colab-logo.png"
              alt="Colab Logo"
            />
          </p>
        </section>

        <section>
          <h2>Jupyter Notebook</h2>
          <p>LINK</p>

          <aside class="notes">
            <p>
              At this link you can download a
              <strong>Jupyter Notebooks blueprint</strong> at this link. After
              the download, you can import it into your Jupyter instance or on
              Google Colab.
            </p>
          </aside>
        </section>

        <!-- ===== PREREQUISITES ===== -->
        <section>
          <h2>Requirements</h2>
          <p>To follow along, you will need:</p>
          <ul>
            <li>
              <strong>Google API Key:</strong> get one from
              <a href="https://aistudio.google.com/app/apikey" target="_blank"
                >Google AI Studio*</a
              >
              <img
                class="inline-logo"
                src="imgs/logo/aistudio-logo.png"
                alt="AI Studio Logo"
              />
            </li>
            <ul>
              <li>
                *Google AI Studio is free of charge and do not require a valid
                credit card for the free tier
              </li>
            </ul>
            <li>
              <strong>ICD-11 Chapter 6 Dataset:</strong> get from
              <a
                href="https://drive.google.com/file/d/1ThIsNf1iuns9wlMZmBHOWRI9E6FiVgjQ/view?usp=drive_link"
                target="_blank"
                >Drive</a
              >
              <img
                class="inline-logo"
                src="imgs/logo/drive-logo.png"
                alt="Drive Logo"
              />
            </li>
          </ul>

          <aside class="notes">
            <p>
              Concerning the <strong>requirements</strong>, you'll need two
              things: a <strong>Google API key</strong> for the models, which
              will be used. The models are free of charge. You don't need a
              subscription or a valid credit card. (Grab it from AI Studio.) and
              the <strong>Chapter 6 CSV</strong>, which we've already placed in
              the Google Drive.
            </p>
          </aside>
        </section>

        <section>
          <h2>
            Google API Key
            <img
              class="inline-logo"
              src="imgs/logo/aistudio-logo.png"
              alt="AI Studio Logo"
            />
          </h2>
          <iframe
            width="560"
            height="315"
            src="https://www.youtube.com/embed/T1BTyo1A4Ww?end=37"
            title="YouTube video player"
            frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen
          ></iframe>

          <aside class="notes">
            <p>
              I want to underline that the
              <strong>API key is free of charge</strong>. You don't need a
              subscription or a valid credit card to use our service.
            </p>
          </aside>
        </section>

        <section>
          <h2>ICD-11 Chapter 6 Dataset</h2>
          <img src="imgs/screenshot.png" />

          <aside class="notes">
            You also have to download the ICD-11 Chapter 6 dataset from the
            drive.
          </aside>
        </section>

        <section>
          <h2>ICD-11 Chapter 6 dataset</h2>
          <ul>
            <li>Total Entries: <strong>720</strong></li>
            <li>
              Most Frequent Category: <strong>6A60</strong> (appearing 18 times)
            </li>
            <li>Average Title Length: <strong>55 characters</strong></li>
            <li>Average Definition Length: <strong>660 characters</strong></li>
          </ul>

          <aside class="notes">
            <p>
              This slide gives a quick overview of the
              <strong>dataset</strong>. It contains
              <strong>720 entries</strong> in total. On average, the
              <strong>titles</strong> of the entries are about
              <strong>55 characters</strong> long, while the
              <strong>definitions</strong> are much longer, with an average of
              <strong>660 characters</strong>.
            </p>
          </aside>
        </section>

        <section>
          <h2>Column Definitions</h2>
          <h3>Core Identification Fields</h3>
          <dl>
            <dt>code</dt>
            <dd>
              The unique alphanumeric identifier for a specific disease or
              disorder
            </dd>
            <dt>title</dt>
            <dd>The official, human-readable name of the condition</dd>
            <dt>definition</dt>
            <dd>A detailed clinical description of the disorder</dd>
          </dl>

          <aside class="notes">
            <p>
              These are the three fundamental fields that every
              <strong>ICD-11 entry</strong> must have. The
              <strong>code</strong> serves as a unique identifier for each
              condition, while the <strong>title</strong> provides a clear and
              understandable name. The <strong>definition</strong> provides the
              detailed medical description that helps clinicians understand
              precisely what this condition entails.
            </p>
          </aside>
        </section>

        <!-- Column Definitions - Part 2: Classification Fields -->
        <section>
          <h2>Column Definitions</h2>
          <h3>Classification & Scope Fields</h3>
          <dl>
            <dt>inclusions</dt>
            <dd>
              Other conditions or synonyms that are included within this
              diagnostic category
            </dd>
            <dt>exclusions</dt>
            <dd>
              Conditions that are explicitly NOT diagnosed under this code
            </dd>
          </dl>

          <aside class="notes">
            <p>
              These fields help clarify the <strong>boundaries</strong> of each
              diagnostic category. <strong>Inclusions</strong> inform us about
              other conditions that fall under the same code.
              <strong>Exclusions</strong> are equally important, as they
              explicitly state what should not be diagnosed with this code,
              helping to prevent misdiagnosis.
            </p>
          </aside>
        </section>

        <!-- Column Definitions - Part 3: Diagnostic & Organizational Fields -->
        <section>
          <h2>Column Definitions</h2>
          <h3>Diagnostic & Organisational Fields</h3>
          <dl>
            <dt>diagnosticCriteria</dt>
            <dd>
              The specific criteria that must be met for a definitive diagnosis
            </dd>
            <dt>category_code</dt>
            <dd>
              A broader, parent-level category that groups together related
              codes
            </dd>
          </dl>

          <aside class="notes">
            <p>
              The <strong>diagnostic criteria</strong> field is crucial for
              clinical practice, as it provides a specific checklist that
              healthcare providers must follow to make an accurate diagnosis.
              The <strong>category code</strong> helps organise the vast number
              of conditions into logical groups, making it easier to navigate
              and understand the relationships between different disorders.
            </p>
          </aside>
        </section>

        <section>
          <h2>TIMER</h2>
          <div id="countdown-timer" class="countdown-animated">05:00</div>
          <div
            id="timer-end-message"
            style="
              display: none;
              font-size: 2em;
              color: #e74c3c;
              text-align: center;
              margin-top: 20px;
              font-weight: bold;
            "
          >
            Time is over!
          </div>
          <aside class="notes">
            We leave you 5 minutes to obtain the API key and download the
            dataset.
          </aside>
        </section>

        <!-- ===== OUR TOOLS: THE LIBRARIES ===== -->
        <section>
          <h2>CODE 1 - Setting up libraries</h2>
          <p>
            Python's power comes from its vast collection of
            libraries—pre-packaged code that we can use. Here are the key ones
            for our project:
          </p>
          <ul>
            <li>
              LangChain
              <img
                class="inline-logo"
                src="imgs/logo/langchain-logo.png"
                alt="LangChain Logo"
              />: The main framework for creating applications with LLMs
            </li>
          </ul>

          <aside class="notes">
            <p>
              <strong>LangChain</strong> is an open-source framework designed to
              simplify the development of applications that use
              <strong>large language models (LLMs)</strong>. It enables
              developers to create powerful and flexible systems that integrate
              LLMs with external data sources, APIs, and custom logic. At its
              core, LangChain facilitates the exchange of information between
              the model and the environment. It provides modular components for
              key functions such as <strong>prompt management</strong>,
              <strong>memory</strong>,
              <strong>chaining multiple operations</strong>, and integration
              with tools like search engines, databases, or user-defined
              functions. One of LangChain's main strengths is its ability to
              "chain" together different steps of reasoning or data
              processing—hence the name. For example, an application might
              retrieve relevant documents, summarise them, and then answer a
              user's question using the summarised content—all orchestrated
              through LangChain. LangChain is widely utilised in real-world
              applications, including <strong>intelligent agents</strong>,
              <strong>chatbots</strong>, <strong>research assistants</strong>,
              and <strong>knowledge-driven interfaces</strong>. It supports
              multiple programming environments, with Python and JavaScript
              being the most popular.
            </p>
          </aside>
        </section>

        <!-- ===== OUR TOOLS: THE LIBRARIES ===== -->
        <section>
          <h2>Setting up libraries</h2>
          <ul>
            <li>
              <strong
                >Pandas
                <img
                  class="inline-logo"
                  src="imgs/logo/pandas-logo.png"
                  alt=""
                />:</strong
              >
              A powerful tool for reading and manipulating data, like our CSV
              file
            </li>
            <li>
              <strong
                >Chroma
                <img
                  class="inline-logo"
                  src="imgs/logo/chroma-logo.png"
                  alt=""
                />:</strong
              >
              A special database (vector store) for storing and searching our
              text data efficiently
            </li>
            <li>
              <strong
                >Google GenAI
                <img
                  class="inline-logo"
                  src="imgs/logo/google-logo.png"
                  alt=""
                />:</strong
              >
              The library that lets us connect to and use Google's powerful AI
              models
            </li>
          </ul>

          <aside class="notes">
            <p>
              <strong>Pandas</strong> is a fast, powerful, and flexible
              open-source data analysis and manipulation library for Python. It
              provides easy-to-use data structures, such as
              <strong>DataFrame</strong> and <strong>Series</strong>, which
              allow users to clean, transform, analyse, and visualise structured
              data efficiently. Pandas is widely used in
              <strong>data science</strong>, <strong>machine learning</strong>,
              and <strong>statistical analysis</strong> thanks to its intuitive
              syntax and strong integration with other Python libraries.
              <strong>Chroma</strong> is a vector database developed by
              Langchain. Chroma allows you to store and search for text data
              efficiently. It is designed to be used with large language models
              (LLMs) and other AI applications that require processing and
              retrieving text data. <strong>Google GenAI</strong> is a library
              that allows you to connect to and use Google's platform as a
              service LLMs so you can query the model in the cloud. It provides
              a simple interface for interacting with these models, making it
              easy to build and deploy AI applications.
            </p>
          </aside>
        </section>

        <section>
          <h2>Setting up libraries</h2>
          <p>
            We use <code><span class="badge">>pip</span></code> to install
            LangChain and its related packages, including the integration for
            Google's AI models and the Chroma
            <img class="inline-logo" src="imgs/logo/chroma-logo.png" alt="" />
            vector store
          </p>
          <pre>
                <code data-line-numbers="1|2|3|4|5"  data-trim data-noescape>
                  pip install requests
                  pip install langchain langchain-community 
                  pip install pandas
                  pip install langchain-chroma
                  pip install langchain-google-genai
                </code>
              </pre>
        </section>

        <section>
          <h2>
            What is an <code><span class="badge">import</span></code> statement?
          </h2>
          <p>
            Think of libraries as toolboxes. An
            <code><span class="badge">import</span></code> statement is how we
            tell Python to open a toolbox and take out a specific tool to use
          </p>
          <pre>
                      <code data-trim data-line-numbers="1-2|4-5">
                        # This line says: "From the langchain toolbox, get me the 'Document' tool."
                        from langchain_core.documents import Document
      
                        # Now we can use the 'Document' tool in our code.
                        my_doc = Document(page_content="This is a sample document.")
                    </code>
                  </pre>
          <p>
            Without importing, Python wouldn't know what a
            <code><span class="badge">Document</span></code> is
          </p>

          <aside class="notes">
            <p>
              This slide explains one of the most basic but essential concepts
              in Python: the <strong>import statement</strong>. You can think of
              libraries as <strong>toolboxes</strong>—collections of useful
              tools that aren't built into the core language. When we write an
              <strong>import statement</strong>, we're telling Python which
              toolbox to open and which specific tool we want to use. For
              example, in the code shown here, we're importing the
              <strong>Document</strong> class from the
              <strong>langchain_core.documents</strong> module. Once imported,
              we can use it to create a new document, like in the line
              <strong>my_doc = Document(...)</strong>. This is the foundation
              for using any external library in Python.
            </p>
          </aside>
        </section>

        <section>
          <h2>Securing Credentials</h2>
          <p>
            Hard-coding sensitive information like API keys directly in the
            source code is a major security risk. A best practice is to load
            them from environment variables, which keeps them separate from the
            code
          </p>

          <aside class="notes">
            <p>
              This slide highlights a very important
              <strong>security practice</strong>. When working with APIs, it
              might seem convenient to paste your API key directly into the
              code, but that's actually a serious risk—especially if the code is
              ever shared or uploaded to a public repository. Instead, we should
              store credentials in <strong>environment variables</strong>. This
              keeps sensitive data out of the codebase and makes our projects
              much safer and easier to manage across different environments.
            </p>
          </aside>
        </section>

        <section>
          <h2>Configuring the API Key</h2>
          <p>
            It's time to write our first lines of Python. The most crucial first
            step is to authenticate our script with Google's AI services. This
            code block handles that by securely loading our API key, which is
            like a password for our program
          </p>
          <pre>
            <code data-line-numbers="1-3|4" data-trim data-noescape>
              import os
              import getpass
            
              os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API Key: ")
            </code>
          </pre>
        </section>

        <section>
          <h2>Loading External Data</h2>
          <p>
            The first step in the
            <span class="gradient-title">RAG</span> process is "Retrieval". This
            requires loading our external knowledge into a format the
            application can use. LangChain provides
            <code><span class="badge">Document</span></code> objects for this,
            which contain text
            <code><span class="badge">page_content</span></code> and associated
            metadata
          </p>
          <p>
            We will write a custom function using
            <code><span class="badge">pandas</span></code> to read our specific
            CSV file and transform each row into a
            <code><span class="badge">LangChain Document</span></code>
          </p>

          <aside class="notes">
            <p>
              The first step in the <strong>RAG pipeline</strong> is
              <strong>retrieval</strong>, which involves bringing external
              knowledge into a usable format. <strong>LangChain</strong> handles
              this using <strong>Document objects</strong>, which store the
              content we want to use, in our case, the text from a CSV file—in
              the <strong>page_content</strong> field, along with any necessary
              metadata. We'll use <strong>pandas</strong> to read our CSV and
              write a function that converts each row into a
              <strong>LAngChain Document</strong>. This prepares our data so it
              can be indexed and retrieved when the language model needs it
              during a query.
            </p>
          </aside>
        </section>

        <section>
          <h2>Why Split Text for Retrieval?</h2>
          <p>
            Once raw documents are loaded, they often need to be broken down
            into smaller, manageable chunks. This is crucial for effective
            retrieval because:
          </p>
          <p>
            The goal is to split text intelligently, maintaining semantic
            coherence within each chunk while ensuring no crucial information is
            cut off mid-sentence or mid-paragraph
          </p>

          <aside class="notes">
            <p>
              Once we've loaded our data into <strong>Document objects</strong>,
              we usually need to split the text into smaller pieces. This is
              essential for making <strong>retrieval</strong> more efficient and
              accurate. Long documents can be difficult for the model to process
              as a whole, so <strong>chunking</strong> helps. But it's not just
              about cutting the text at fixed intervals—what we really want is
              to split it in a smart way that keeps each chunk meaningful.
              Ideally, we avoid breaking sentences or paragraphs so the model
              can work with complete, coherent bits of information during
              retrieval.
            </p>
          </aside>
        </section>

        <section>
          <h2>The Goal of Splitting</h2>
          <ul>
            <li>
              <strong>Relevance:</strong> LLMs have context windows. Sending too
              much irrelevant information can dilute the useful context. Smaller
              chunks mean the retriever finds more
              <strong>precise</strong> relevant passages
            </li>
            <li>
              <strong>Efficiency:</strong> Embedding and retrieving smaller
              chunks is faster and uses less memory
            </li>
            <li>
              <strong>Granularity:</strong> Ensures that retrieved information
              is at the right level of detail for the LLM to synthesize
            </li>
          </ul>

          <aside class="notes">
            <p>
              Splitting text isn't just about making things smaller—it's about
              improving the overall performance of the
              <strong>retrieval system</strong>. First, there's
              <strong>relevance</strong>: LLMs have limited context windows, so
              we want to avoid flooding them with unnecessary content. Smaller,
              well-focused chunks help retrieve only the most relevant pieces.
              Next is <strong>efficiency</strong>: Working with smaller chunks
              speeds up embedding and retrieval processes and reduces memory
              usage. And finally, <strong>granularity</strong>: Properly sized
              chunks ensure the information passed to the model is at the right
              level of detail—not too broad, not too narrow—making it easier for
              the model to generate meaningful and accurate responses.
            </p>
          </aside>
        </section>

        <section>
          <h2>CODE 2 - Implementing the CSV Loader</h2>
          <p>
            Now implement a function that reads the
            <code><span class="badge">icdchapter6.csv</span></code> file,
            iterates through each row, and creates a
            <code><span class="badge">Document</span></code> object with
            formatted content and metadata
          </p>
          <p>Follow the bluprint in the Jupyter</p>
        </section>

        <section
          data-background-image="https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExcHR5eGJoN2g4OTdvaTZ4ZzZ3OWh6ZmQ0aWo5Mmw3aTh0dWgxempyaCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/VekcnHOwOI5So/giphy.gif"
        >
          <h2>TIMER</h2>
          <div id="countdown-timer" class="countdown-animated">05:00</div>
          <div
            id="timer-end-message"
            style="
              display: none;
              font-size: 2em;
              color: #e74c3c;
              text-align: center;
              margin-top: 20px;
              font-weight: bold;
            "
          >
            Time is over!
          </div>
          <aside class="notes">
            We leave you 5 minutes to implement the code.
          </aside>
        </section>

        <section>
          <h2>CODE 2 - Implementing the CSV Loader</h2>
        </section>

        <section>
          <pre><code data-line-numbers="1-2|4-14|16-18" data-trim data-noescape>
                import pandas as pd
                from langchain_core.documents import Document

                def load_icd11_from_csv(file_path: str) -> list[Document]:
                      df = pd.read_csv(file_path, delimiter=';')
                      icd_documents = []
                      for index, row in df.iterrows():
                        content = (f"ICD-11 Code: {row['code']}\\n"
                              f"Title: {row['title']}\\n"
                              f"Definition: {row['definition']}")
                        metadata = {"code": row['code'], "title": row['title']}
                        doc = Document(page_content=content, metadata=metadata)
                        icd_documents.append(doc)
                      return icd_documents
                    
                #  Let's load the documents
                icd11_documents = load_icd11_from_csv('icdchapter6.csv')
                print(f"Loaded {len(icd11_documents)} documents.")
              </code></pre>
        </section>

        <section>
          <h2>Why These Data Fields? Structuring the Context</h2>
          <p>
            In our loader, we combined the <strong>code</strong>,
            <strong>title</strong>, and <strong>definition</strong> for a reason
          </p>
          <ul>
            <li>
              <strong>Code & Title:</strong> These provide clear, structured
              identifiers. They are perfect for when the LLM needs to give a
              precise, factual answer
            </li>
          </ul>

          <aside class="notes">
            <p>
              When preparing our data for retrieval, we deliberately chose to
              combine the <strong>code</strong>, <strong>title</strong>, and
              <strong>definition</strong> fields. This wasn't random—each part
              plays a key role. The <strong>code and title</strong> serve as
              strong identifiers. They're concise, structured, and ideal when
              the model needs to respond with factual information—like
              recognising a diagnostic label. Including them helps the LLM
              quickly anchor the context and deliver accurate answers,
              especially when precision matters.
            </p>
          </aside>
        </section>

        <section>
          <h2>Why These Data Fields? Structuring the Context</h2>
          <ul>
            <li>
              <strong>Definition:</strong> This is the most important part for
              our retriever. It contains the rich, descriptive text that allows
              for effective <strong>semantic search</strong>, helping the system
              find relevant concepts even if the query doesn't use exact
              keywords
            </li>
          </ul>
          <aside class="notes">
            <p>
              The <strong>definition</strong> is by far the most valuable part
              of each entry when it comes to retrieval. It's where the real
              descriptive content lives—rich enough to carry the meaning of the
              concept. This is crucial for <strong>semantic search</strong>,
              where we rely on meaning rather than exact words. Even if the
              user's query is phrased differently, the system can still retrieve
              the right content because it's matching based on meaning, not just
              keywords. That's why including <strong>definitions</strong> is
              essential for making our retrieval accurate and flexible.
            </p>
          </aside>
        </section>

        <section>
          <h2>Why These Data Fields? Structuring the Context</h2>
          <p>
            By combining them, we create a comprehensive
            <code><span class="badge">Document</span></code> that gives the LLM
            everything it needs: a unique ID, a clear label, and a detailed
            description to inform its answer
          </p>
          <aside class="notes">
            <p>
              By combining them, we produce a comprehensive document that
              provides the LLM with everything it requires: a
              <strong>unique ID</strong>, a <strong>clear label</strong>, and a
              <strong>detailed description</strong> to inform its answer.
            </p>
          </aside>
        </section>

        <section>
          <h2 class="r-fit-text checkpoint-animate">Lab Checkpoint 1</h2>
          <pre>
            <code data-line-numbers="1-4|6-9|11-21|23-29" data-trim data-noescape>
              import os, getpass
              from dotenv import load_dotenv
              import pandas as pd
              from langchain_core.documents import Document
              
              # API KEY SETUP
              load_dotenv()
              if "GOOGLE_API_KEY" not in os.environ:
                os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter Google API Key: ")
              
              # DATA LOADING FUNCTION
              def load_icd11_from_csv(file_path: str) -> list[Document]:
                try:
                  df = pd.read_csv(file_path, delimiter=';'); docs = []
                  for i, row in df.iterrows():
                    c = (f"ICD-11 Code: {row['code']}\\n"
                      f"Title: {row['title']}\\n"
                      f"Definition: {row['definition']}")
                    docs.append(Document(page_content=c, metadata={"code": row['code'], "title": row['title']}))
                  return docs
                except FileNotFoundError: return []
              
              # EXECUTE LOADING
              icd11_documents = load_icd11_from_csv('icdchapter6.csv')
              if icd11_documents:
                print(f"Loaded {len(icd11_documents)} documents.")
                icd11_chunks = icd11_documents
              else:
                print("Failed to load documents.")
						  </code>
            </pre>
        </section>

        <section>
          <h2 class="title-animate">
            <span class="selective-yellow">Part 3</span>: Embeddings & Vector
            Stores
          </h2>

          <aside class="notes">
            <p>
              In this section, we'll move from <strong>raw data</strong> to a
              format that can be <strong>efficiently searched</strong>.
            </p>
          </aside>
        </section>

        <section>
          <h2>Embeddings & Vector Stores</h2>
          <p>Now that we have our data loaded, we need to make it searchable</p>
          <p>
            We'll do this by converting our text into numerical representations
            and storing them in a specialized database
          </p>

          <aside class="notes">
            <p>
              At this point, we've loaded and structured our data, but it's
              still just <strong>plain text</strong>. Now we need to make it
              <strong>searchable</strong>.
            </p>
          </aside>
        </section>

        <section>
          <h2>Creating Text Embeddings</h2>
          <div class="content">
            <div class="theory-block">
              <p class="main-point">
                To find relevant information, we can't just match keywords. We
                need to understand the <strong>meaning</strong> or
                <strong>semantic content</strong> of the text
              </p>
              <p class="detail-point">
                <strong>Embeddings</strong> are numerical vectors that represent
                this semantic meaning. An embedding model converts our text
                documents into these vectors, placing similar concepts close to
                each other in vector space
              </p>
            </div>
          </div>

          <aside class="notes">
            <p>
              <strong>Keyword matching</strong> isn't enough. We need the system
              to understand the <strong>meaning</strong>—the
              <strong>semantic content</strong>—of a question and find texts
              that are conceptually related, even if the wording is different.
              That's where <strong>embeddings</strong> come in. These are
              numerical vectors that capture the semantic meaning of a text. An
              <strong>embedding model</strong> transforms each chunk of text
              into a vector, and similar meanings end up close together in
              vector space. So instead of searching by words, we're searching by
              <strong>meaning</strong>—which makes the retrieval process much
              more powerful and flexible.
            </p>
          </aside>
        </section>

        <section>
          <h2>A Note on Debugging</h2>
          <p>
            Errors are a normal part of coding! <strong>Debugging</strong> is
            the process of finding and fixing them. No one writes perfect code
            on the first try.
          </p>
          <p>
            The simplest yet most powerful tool in your debugging toolkit is the
            <code><span class="badge">print()</span></code> statement. If you're
            not sure what a variable holds or if a line of code is being
            reached, just print it! In Colab, the output will appear right below
            the code cell.
          </p>
        </section>

        <!-- ===== A NOTE ON DEBUGGING ===== -->
        <section>
          <h2>A Note on Debugging</h2>
          <p>You can print text, variables or both. Here are some examples:</p>
          <pre>
          <code data-line-numbers="1|2|3" data-trim data-noescape>
            print("just a literal text")
            print(variable_name)
            print(f"String interpolation: {variable_name}")
          </code>
        </pre>
        </section>

        <section>
          <h2>CODE 3 - Generating Embeddings</h2>
          <p>
            We'll use Google's
            <code><span class="badge">embedding-001</span></code> model via the
            <code><span class="badge">GoogleGenerativeAIEmbeddings</span></code>
            class in LangChain to perform this conversion
          </p>
          <pre>
            <code data-line-numbers="1|3-6|7" data-trim data-noescape>
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
		
            # Initialize the embedding model
            embeddings = GoogleGenerativeAIEmbeddings(
              model="models/embedding-001"
            )
            print("Embedding model initialized.")
            </code>
          </pre>

          <aside class="notes">CITE TRY CATCH</aside>
        </section>

        <!-- ===== THEORY: INDEXING IN A VECTOR STORE ===== -->
        <section>
          <h2>Indexing in a Vector Store</h2>
          <p>
            Searching through thousands of embeddings one-by-one would be very
            slow. A <strong>Vector Store</strong> is a specialized database
            designed to store and efficiently search these high-dimensional
            vectors using fast algorithms like Approximate Nearest Neighbor
            (ANN) search
          </p>
          <p>
            We will use
            <strong>Chroma</strong>
            <img class="inline-logo" src="imgs/logo/chroma-logo.png" alt="" />,
            a popular open-source vector store that runs in memory but can be
            persisted to disk to avoid re-processing
          </p>

          <aside class="notes">
            <p>
              Once we've created our <strong>embeddings</strong>, we need an
              efficient way to search through them. Doing a linear search over
              thousands of vectors would be too slow, especially in real-time
              applications. That's why we use a <strong>Vector Store</strong>—a
              specialised database optimised for handling high-dimensional
              vectors. It uses fast algorithms like
              <strong>Approximate Nearest Neighbor (ANN) search</strong> to
              quickly find vectors that are close in meaning to a query. In this
              lab, we'll use <strong>Chroma</strong>, an open-source vector
              store that works in memory for speed but can also be saved to disk
              to avoid recalculating embeddings every time.
            </p>
          </aside>
        </section>

        <section>
          <h2>CODE 4 - Creating or Loading a Persistent Vector Store</h2>
          <p>
            This code creates a new Chroma database from our documents and
            embeddings if one doesn't already exist, or loads the existing one
            from disk
          </p>
        </section>

        <section>
          <pre>
            <code data-line-numbers="1-2|4|6-11|12-17|19-21" data-trim data-noescape>
              from langchain_chroma import Chroma
              import os
                  
              persist_directory = "./chroma_db_workshop"
                  
              # Load from disk if it exists, otherwise create it
              if os.path.exists(persist_directory):
                vectorstore = Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
                )
              else:
                vectorstore = Chroma.from_documents(
                      documents=icd11_chunks,
                      embedding=embeddings,
                      persist_directory=persist_directory
                    )
                  
              # The retriever is our interface for searching
              retriever = vectorstore.as_retriever()
              print("Retriever is ready.")
								</code>
              </pre>
        </section>

        <!-- ===== LAB CHECKPOINT 2 ===== -->
        <section>
          <h2 class="r-fit-text checkpoint-animate">LAB Checkpoint 2</h2>
          <pre>
            <code data-line-numbers data-trim data-noescape>
                # ... (previous code from Checkpoint 1) ...
                from langchain_google_genai import GoogleGenerativeAIEmbeddings
                from langchain_chroma import Chroma
                
                # ... (API Key and Data Loading code) ...
                icd11_chunks = load_icd11_from_csv('icdchapter6.csv')
                print(f"Loaded {len(icd11_chunks)} chunks.")
                
                # EMBEDDING MODEL
                embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
                
                # VECTOR STORE
                persist_directory = "./chroma_db_workshop"
                if os.path.exists(persist_directory) and os.listdir(persist_directory):
                  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
                  print("Loaded existing vector store.")
                else:
                  vectorstore = Chroma.from_documents(documents=icd11_chunks, embedding=embeddings, persist_directory=persist_directory)
                  print("Created new vector store.")
                
                retriever = vectorstore.as_retriever()
                print("Retriever is ready.")
								 </code></pre>
        </section>

        <!-- ===== LAB CHECKPOINT 2 ===== -->
        <section
          data-background-image="https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExdmI0aTYxenRoM2xhZXhkMG4wNzgzaG83Nnh4NXA1NzJ5eGc4bTNsMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/oYQ9HRm5Mo7VXeMNVR/giphy.gif"
        >
          <h2>BREAK</h2>
          <div id="countdown-timer" class="countdown-animated">05:00</div>
          <div
            id="timer-end-message"
            style="
              display: none;
              font-size: 2em;
              color: #e74c3c;
              text-align: center;
              margin-top: 20px;
              font-weight: bold;
            "
          >
            Time is over!
          </div>
        </section>

        <section>
          <h2 class="title-animate">
            <span class="tangerine">Part 4</span>: Building & Querying the RAG
            Chain
          </h2>
        </section>

        <!-- ===== RAG ===== -->
        <section>
          <h2>Building & Querying the RAG Chain</h2>
          <p>
            Now we have all the components: a retriever to fetch data and an LLM
            to generate answers. The final step is to orchestrate them into a
            single, cohesive application
          </p>

          <aside class="notes">
            <p>
              Now that we've built all the components—the
              <strong>retriever</strong> for fetching relevant chunks and the
              <strong>LLM</strong> for generating answers—the final step is to
              combine them into a single pipeline. This is what we call the
              <strong>RAG chain</strong>: it orchestrates the interaction
              between <strong>retrieval</strong> and
              <strong>generation</strong>, allowing us to ask natural-language
              questions and receive answers that are grounded in our indexed
              knowledge base. This step transforms our setup into a fully
              functional application.
            </p>
          </aside>
        </section>

        <section>
          <h2>Composing with LCEL</h2>
          <p>
            <strong>LangChain Expression Language (LCEL)</strong> is a
            declarative way to compose components into chains. The pipe (`|`)
            operator connects each step, passing the output of one step as the
            input to the next
          </p>
          <div>
            <p>Our chain will be:</p>
            <ol>
              <li>The user's question retrieves context</li>
              <li>The question and context populate a prompt</li>
              <li>The prompt is sent to the LLM</li>
              <li>The LLM's response is parsed into a clean string</li>
            </ol>
          </div>

          <aside class="notes">
            <p>
              To build the <strong>RAG chain</strong>, we'll use
              <strong>LangChain Expression Language (LCEL)</strong>. It's a
              declarative syntax that lets us compose chains using the
              <strong>| operator</strong>—just like a pipeline. This makes it
              easy to connect components, where the output of one step becomes
              the input of the next. In our case, the chain is made of four
              parts: The user's question triggers the
              <strong>retriever</strong>, The question and retrieved context are
              combined into a <strong>prompt</strong>, The prompt is passed to
              the <strong>LLM</strong>, And the LLM's response is parsed into a
              clean string that we can show back to the user.
              <strong>LCEL</strong> makes this structure simple and readable,
              which is great for rapid development and debugging.
            </p>
          </aside>
        </section>

        <section>
          <h2>
            How <span class="gradient-title">RAG</span> Uses a Prompt Template
          </h2>
          <p>
            We don't write a new prompt for every user question. Instead, we use
            a <strong>template</strong>. LangChain dynamically inserts the
            retrieved context and the user's original question into the
            placeholders of this template
          </p>
          <pre>
            <code data-line-numbers data-trim data-noescape>
              # Our template has two placeholders: {context} and {question}
              template = """Use the retrieved context to answer the question.

              Context: {context}
              Question: {question}"""

              # LangChain will automatically fill these in before sending to the LLM
            </code>
            </pre>

          <aside class="notes">
            <p>
              We don't write a new prompt every time a user asks a question.
              Instead, we define a <strong>template</strong> with
              placeholders—in our case, <strong>{context}</strong> and
              <strong>{question}</strong>.
              <strong>LangChain</strong> automatically fills in those
              placeholders at runtime: it inserts the retrieved text and the
              user's query, then sends the full prompt to the
              <strong>LLM</strong>. This approach ensures
              <strong>consistency</strong>, makes the chain easier to maintain,
              and lets us easily tweak or improve the prompt without rewriting
              our logic.
            </p>
          </aside>
        </section>

        <!-- ===== GIVING CLEAR INSTRUCTIONS ===== -->
        <section>
          <h2>Giving Clear Instructions</h2>
          <p>
            The most important part of our prompt is the instructions that guide
            the LLM's behavior. Consider this line:
          </p>
          <p style="text-align: center; margin: 20px 0; font-style: italic">
            "If you don't know the answer, just say that you don't know."
          </p>
          <p>
            This single instruction is critical for building a trustworthy
            system. It explicitly tells the LLM not to guess or "hallucinate" an
            answer if the information isn't in the retrieved context. This makes
            our RAG system more reliable and factual
          </p>

          <aside class="notes">
            <p>
              The most important part of our prompt isn't the context or the
              question—it's the <strong>instructions</strong> we give to the
              LLM. These guide how it should behave. A great example is the
              line:
              <strong
                >"If you don't know the answer, just say that you don't
                know."</strong
              >
              This simple instruction is crucial. It prevents the model from
              guessing or hallucinating answers when it doesn't have enough
              information. Instead, we force it to stay grounded in the
              retrieved content. Adding this makes our system more
              <strong>reliable</strong>, <strong>transparent</strong>, and
              <strong>trustworthy</strong>, which is essential when working with
              sensitive data like medical knowledge.
            </p>
          </aside>
        </section>

        <section>
          <h2>CODE 5 - Building the RAG Chain</h2>
          <p>
            We define a prompt template and then pipe together the retriever,
            the prompt, the LLM, and an output parser.
          </p>
        </section>

        <section>
          <pre>
              <code data-line-numbers="1-4|6-7|9-16|17|20-25|27" data-trim data-noescape>
                  from langchain_google_genai import ChatGoogleGenerativeAI
                  from langchain_core.prompts import ChatPromptTemplate
                  from langchain_core.output_parsers import StrOutputParser
                  from langchain_core.runnables import RunnablePassthrough
                  
                  # Initialize the Language Model (LLM)
                  llm = ChatGoogleGenerativeAI(model="gemma-3-1b-it", temperature=0.7)
                  
                  # Define the prompt template
                  template = """You are a helpful assistant for ICD-11.
                  Use the retrieved context to answer the question.
                  If you don't know the answer, just say that you don't know.
                  
                  Context: {context}
                  Question: {question}
                  Answer:"""
                  prompt = ChatPromptTemplate.from_template(template)
                  
                  # Construct the RAG chain with LCEL
                  rag_chain = (
                    {"context": retriever, "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
                  )
                  
                  print("RAG chain constructed.")
								</code>
              </pre>
        </section>

        <section>
          <h2 class="title-animate">
            <span class="persimmon">Part 5</span>: Querying the RAG System
          </h2>
        </section>

        <!-- ===== THEORY: QUERYING THE SYSTEM ===== -->
        <section>
          <h2>Querying the System</h2>
          <p>
            The <code><span class="badge">rag_chain</span></code> we created is
            now a runnable object. We can use its
            <code><span class="badge">invoke()</span></code> method to pass in a
            user query. This triggers the entire sequence of operations we
            defined, returning the final, context-aware answer from the LLM
          </p>

          <aside class="notes"></aside>
        </section>

        <section>
          <h3>Putting it to the Test</h3>
          <p>
            We define a helper function and then ask our RAG system a question
            that is inside its knowledge base and one that is outside of it.
          </p>
        </section>

        <section>
          <pre>
                      <code data-line-numbers="1|2|3-4|6-7|9-10" data-trim data-noescape>
                          def ask_rag_system(query: str):
                            print(f"\\n--- Asking: '{query}' ---")
                            response = rag_chain.invoke(query)
                            print(f"Answer:\\n{response}")
                          
                          # Query 1: Specific question
                          ask_rag_system("What is the ICD-11 code for 'Dissociative identity disorder'?")
                          
                          # Query 2: Off-topic question
                          ask_rag_system("What is the capital of France?")
                        </code>
                      </pre>
        </section>

        <section
          data-background-image="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExdmI0aTYxenRoM2xhZXhkMG4wNzgzaG83Nnh4NXA1NzJ5eGc4bTNsMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ebFG4jcnC1Ny8/giphy.gif"
        >
          <h2 class="r-fit-text">Final Code: <br />Complete RAG System</h2>
        </section>

        <section>
          <pre>
              <code data-line-numbers="1-9|11-12|14-23|25-28|30-34|36-39|41-42" data-trim data-noescape>
                  import os, getpass
                  from dotenv import load_dotenv
                  import pandas as pd
                  from langchain_core.documents import Document
                  from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
                  from langchain_chroma import Chroma
                  from langchain_core.prompts import ChatPromptTemplate
                  from langchain_core.output_parsers import StrOutputParser
                  from langchain_core.runnables import RunnablePassthrough
                  
                  if "GOOGLE_API_KEY" not in os.environ:
                    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter Google API Key: ")
                  
                  # Data Loading ... (condensed function)
                  def load_icd11_from_csv(file_path: str): 
                    try:
                      df=pd.read_csv(file_path,delimiter=';');docs=[]
                      for i, r in df.iterrows():
                        c = f"Code: {r['code']}\\nTitle: {r['title']}\\nDef: {r['definition']}"
                        docs.append(Document(page_content=c, metadata={"code": r['code']}))
                      return docs
                    except FileNotFoundError: return []
                  icd11_chunks = load_icd11_from_csv('icdchapter6.csv')
                  
                  # Embeddings & Vector Store
                  embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
                  
                  # VECTOR STORE
                  persist_directory = "./chroma_db_workshop"
                  if os.path.exists(persist_directory) and os.listdir(persist_directory):
                    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
                    print("Loaded existing vector store.")
                  else:
                    vectorstore = Chroma.from_documents(documents=icd11_chunks, embedding=embeddings, persist_directory=persist_directory)
                    print("Created new vector store.")
                  
                  retriever = vectorstore.as_retriever()
                  print("Retriever is ready.")
                  
                  # RAG Chain
                  llm = ChatGoogleGenerativeAI(model="gemma-3-1b-it", temperature=0.7)
                  template = "Context: {context}\\nQuestion: {question}\\nAnswer:"
                  prompt = ChatPromptTemplate.from_template(template)
                  rag_chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
                  
                  # Querying
                  def ask_rag_system(query: str):
                    response = rag_chain.invoke(query)
                    print(f"\\nQuery: {query}\\nAnswer: {response}")
                  
                  ask_rag_system("What is 'Dissociative identity disorder'?")
                  ask_rag_system("What is the capital of France?")
								</code>
              </pre>
        </section>

        <!-- ===== WORKSHOP WRAP-UP ===== -->
        <section>
          <h2>Summary</h2>
          <div>
            <ul class="content-list">
              <li class="list-item">
                <span class="text">Set up a Python environment</span>
              </li>
              <li class="list-item">
                <span class="text">Loaded external knowledge</span>
                <code><span class="badge">Documents</span></code>
              </li>
              <li class="list-item">
                <span class="text">Generated semantic embeddings</span>
              </li>
              <li class="list-item">
                <span class="text">Indexed these embeddings</span>
              </li>
              <li class="list-item">
                <span class="text">Constructed a complete RAG chain</span>
              </li>
              <li class="list-item">
                <span class="text">Queried our system</span>
              </li>
            </ul>
          </div>
        </section>

        <section data-background-color="#fff">
          <h2>LLMind</h2>
          <img
            src="imgs/scheme/rag_pipeline.png"
            style="
              max-width: 80%;
              height: auto;
              margin: 20px auto;
              display: block;
            "
          />

          <aside class="notes">
            <p>
              The pipeline we just explored has also been used to build a tool
              called <strong>LLMind</strong>—a system designed to suggest
              psychiatric diagnoses based on clinical case descriptions. This
              kind of tool is particularly valuable given the complexity and
              nuance involved in psychiatric and psychological assessment.
              Diagnoses often rely on overlapping symptoms, contextual factors,
              and evolving criteria, which makes decision-making both delicate
              and cognitively demanding. By integrating structured knowledge
              from <strong>ICD-11</strong> with the flexibility of language
              models, <strong>LLMind</strong> helps clinicians navigate this
              complexity, offering consistent, transparent, and evidence-based
              support. In expert evaluations, LLMind reached a
              <strong>diagnostic accuracy of 76.1%</strong>, showing promising
              results as a clinical decision support system—not as a
              replacement, but as a smart assistant.
            </p>
          </aside>
        </section>

        <section
          data-background-iframe="https://llmind.datai.disco.unimib.it/"
        ></section>

        <section
          data-background-image="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExdmI0aTYxenRoM2xhZXhkMG4wNzgzaG83Nnh4NXA1NzJ5eGc4bTNsMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/tU2mV8ALzJEdXAAwRo/giphy.gif"
        >
          <h2>Questions&nbsp&nbsp&nbsp&nbsp&nbsp;</h2>
        </section>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      Reveal.initialize({
        view: "scroll",
        scrollProgress: true,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });

      // Reset and replay title animation
      Reveal.on("slidechanged", (event) => {
        const title = event.currentSlide.querySelector(".title-animate");
        if (title) {
          const colorSpan = title.querySelector("span");
          if (colorSpan) {
            const colorClass = colorSpan.className;
            const colorVar = `var(--${colorClass})`;

            // ✅ CORREZIONE: Rimuovi style precedenti
            const existingStyle = document.querySelector('#title-animation-style');
            if (existingStyle) {
              existingStyle.remove();
            }
            const style = document.createElement("style");
            style.id = 'title-animation-style';
            style.textContent = `
              .title-animate::after {
                background: linear-gradient(120deg, ${colorVar} 0%, ${colorVar} 100%) !important;
              }
            `;
            document.head.appendChild(style);
          }

          title.classList.remove("title-animate");
          void title.offsetWidth;
          title.classList.add("title-animate");
        }
      });

      // Add counting animation
      function animateNumber(element, start, end, duration) {
        let startTimestamp = null;
        element.classList.add("active");
        const step = (timestamp) => {
          if (!startTimestamp) startTimestamp = timestamp;
          const progress = Math.min((timestamp - startTimestamp) / duration, 1);
          const current = Math.floor(progress * (end - start) + start);
          element.textContent = current.toLocaleString();
          if (progress < 1) {
            window.requestAnimationFrame(step);
          } else {
            setTimeout(() => {
              element.classList.remove("active");
            }, 500);
          }
        };
        window.requestAnimationFrame(step);
      }

      // Start animation when the slide is shown
      Reveal.on("slidechanged", (event) => {
        if (event.currentSlide.querySelector("#counting-number")) {
          const element = document.getElementById("counting-number");
          animateNumber(element, 0, 85000, 2000);
        }
      });
    </script>
    <script>
      // TIMER MULTIPLI RESETTABILI SU SLIDE SHOW, TEMPO PRESO DAL TESTO DEL DIV
      (function () {
        // Mappa per tenere traccia degli intervalli attivi per ogni timer
        const timerIntervals = {};

        function parseTimeString(timeStr) {
          // Accetta formato mm:ss
          const match = timeStr.match(/^(\d{1,2}):(\d{2})$/);
          if (!match) return 300; // fallback 5 minuti
          return parseInt(match[1], 10) * 60 + parseInt(match[2], 10);
        }

        function startCountdown(timerElem, endMsgElem) {
          if (timerElem.dataset.timerIntervalId) {
            clearInterval(timerIntervals[timerElem.dataset.timerIntervalId]);
            delete timerIntervals[timerElem.dataset.timerIntervalId];
          }
          // Leggi il tempo iniziale dal testo del div
          let duration = parseTimeString(timerElem.textContent.trim());
          // Genera un id unico per questo timer
          const timerId = Math.random().toString(36).substr(2, 9);
          timerElem.dataset.timerIntervalId = timerId;
          let timer = duration;
          timerElem.classList.remove("timer-blink");
          if (endMsgElem) endMsgElem.style.display = "none";
          // Visualizza il tempo iniziale
          let minutes = parseInt(timer / 60, 10);
          let seconds = parseInt(timer % 60, 10);
          minutes = minutes < 10 ? "0" + minutes : minutes;
          seconds = seconds < 10 ? "0" + seconds : seconds;
          timerElem.textContent = minutes + ":" + seconds;
          timerIntervals[timerId] = setInterval(function () {
            let minutes = parseInt(timer / 60, 10);
            let seconds = parseInt(timer % 60, 10);
            minutes = minutes < 10 ? "0" + minutes : minutes;
            seconds = seconds < 10 ? "0" + seconds : seconds;
            timerElem.textContent = minutes + ":" + seconds;
            if (--timer < 0) {
              clearInterval(timerIntervals[timerId]);
              timerElem.textContent = "00:00";
              timerElem.classList.add("timer-blink");
              if (endMsgElem) endMsgElem.style.display = "block";
            }
          }, 1000);
        }

        function setupTimersOnSlide(slide) {
          // Trova tutti i timer nella slide corrente
          const timers = slide.querySelectorAll("#countdown-timer");
          timers.forEach((timerElem) => {
            // Trova il messaggio di fine associato (deve essere fratello o vicino)
            let endMsgElem =
              timerElem.parentNode.querySelector("#timer-end-message");
            startCountdown(timerElem, endMsgElem);
          });
        }

        // All'avvio, se c'è un timer visibile, avvialo
        document.addEventListener("DOMContentLoaded", function () {
          const currentSlide = document.querySelector(
            '.slides section.present, .slides section[aria-selected="true"]'
          );
          if (currentSlide) setupTimersOnSlide(currentSlide);
        });

        // Ogni volta che cambia slide, avvia i timer della slide corrente
        if (window.Reveal) {
          Reveal.on("slidechanged", function (event) {
            if (event.currentSlide) setupTimersOnSlide(event.currentSlide);
          });
        } else {
          document.addEventListener("DOMContentLoaded", function () {
            if (window.Reveal) {
              Reveal.on("slidechanged", function (event) {
                if (event.currentSlide) setupTimersOnSlide(event.currentSlide);
              });
            }
          });
        }
      })();
    </script>
  </body>
</html>
