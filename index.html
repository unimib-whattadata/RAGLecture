<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>AICL Lectures 2025</title>

    <link rel="stylesheet" href="css/custom.css" />
    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" />
    <link rel="stylesheet" href="plugin/highlight/monokai.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.5/gsap.min.js"></script>
    <style>
      .inline-logo {
        height: 1em;
        vertical-align: middle;
        margin-left: 0.3em;
        position: relative;
        top: -1px;
      }
      .badge {
        display: inline-block;
        background: #444;
        color: #fff;
        border-radius: 0.4em;
        padding: 0.1em 0.5em;
        font-size: 0.9em;
        font-family: monospace;
        vertical-align: middle;
      }
      .checkpoint-animate {
        background: linear-gradient(90deg, #ffe259, #ffa751, #ffe259);
        background-size: 200% auto;
        color: #222;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: checkpoint-glow 2s linear infinite;
        font-weight: bold;
        text-shadow: 0 0 10px #ffe25988, 0 0 20px #ffa75144;
      }
      @keyframes checkpoint-glow {
        0% {
          background-position: 0% 50%;
        }
        100% {
          background-position: 100% 50%;
        }
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <!-- Slide: Retrieval Augmented Generation, Large Language Models and Knowledge Bases -->
        <section>
          <img class="r-stretch" src="imgs/AILC_Logo.png" />
          <h2 class="gradient-title">
            Retrieval Augmented Generation, Large Language Models and Knowledge
            Bases
          </h2>
          <p>Building a RAG System with LangChain and <br />ICD-11 Data</p>

          <aside class="notes">
            Good morning everyone and welcome to our hands on workshop Building
            a Retrieval Augmented Generation System with LangChain and ICD-11
            Data. My name is Marco Cremaschi. I'm a researcher at the University
            of Milano-Biccocca and teacher of a database course in the master's
            degree in computational linguistics. Over the next two hours we'll
            explore why RAG matters and set up the environment together.
          </aside>
        </section>

        <!-- ===== SPEAKER ===== -->
        <section>
          <div style="text-align: center">
            <img
              src="imgs/speaker/mc-coloured.png"
              alt="Marco Cremaschi"
              style="max-width: 400px"
            />
            <div style="margin-top: 10px">Marco Cremaschi</div>
          </div>

          <aside class="notes">
            My name is Marco Cremaschi. I'm a researcher at the University of
            Milano-Biccocca and teacher of a database course in the master's
            degree in computational linguistics.
          </aside>
        </section>

        <!-- ===== TUTORS ===== -->
        <section>
          <h2>Tutors</h2>
          <div
            style="
              display: flex;
              justify-content: center;
              align-items: flex-start;
              gap: 60px;
              margin-top: 40px;
            "
          >
            <div style="text-align: center">
              <img
                src="imgs/speaker/dc-coloured.png"
                alt="David Chieregato"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">David Chieregato</div>
            </div>
            <div style="text-align: center">
              <img
                src="imgs/speaker/fd-coloured.png"
                alt="Fabio D'Adda"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">Fabio D'Adda</div>
            </div>
            <div style="text-align: center">
              <img
                src="imgs/speaker/dd-coloured.png"
                alt="Davide Ditolve"
                style="max-width: 200px"
              />
              <div style="margin-top: 10px">Davide Ditolve</div>
            </div>
          </div>
          <aside class="notes">
            For this lab we have also the help of David Chieregato, Fabio
            D'Adda, and Davide Ditolve. They'll walk around during the coding
            checkpoints, so if need any help just raise your hand.
          </aside>
        </section>

        <!-- ===== LAB AGENDA ===== -->
        <section>
          <h2 class="title-animate">
            <span class="maya-blue"></span> LAB Agenda
          </h2>
        </section>

        <!-- ===== LAB AGENDA LIST ===== -->
        <section>
          <ul class="gradient-list">
            <li>
              <span class="tekhelet">Part 1</span>: The Fundamentals (RAG &
              ICD-11)
            </li>
            <li class="fragment">
              <span class="medium-slate-blue">Part 2</span>: Prerequisites,
              Setup, Data Loading & Processing
            </li>
            <li class="fragment">
              <span class="selective-yellow">Part 3</span>: Embeddings & Vector
              Stores
            </li>
            <li class="fragment">
              <span class="tangerine">Part 4</span>: Building the RAG Chain
            </li>
            <li class="fragment">
              <span class="persimmon">Part 5</span>: Querying the RAG System
            </li>
            <li class="fragment">Conclusion & Q&A</li>
          </ul>

          <aside class="notes">
            Everything is broken into five parts that map directly onto the code
            checkpoints. We'll start with the fundamentals, then move through
            setup, embeddings, chain assembly, and finally querying. After the
            wrap-up there's time for Q&A.
          </aside>
        </section>

        <!-- ===== PART 1: THE FUNDAMENTALS ===== -->
        <section id="part-1">
          <h2 class="title-animate">
            <span class="tekhelet">Part 1</span>: The Fundamentals (RAG &
            ICD-11)
          </h2>

          <aside class="notes">
            This first module is purely conceptual: what RAG is and why ICD-11
            makes a perfect demo dataset.
          </aside>
        </section>

        <!-- ===== WHAT IS RAG ===== -->
        <section>
          <h2>What is RAG?</h2>
          <p>
            <span class="gradient-title"
              >Retrieval-Augmented Generation (RAG)</span
            >
            is an AI framework for improving the quality of LLM-generated
            responses by grounding the model on external sources of knowledge
          </p>
          <p>
            It combines a retriever (to find relevant information) with a
            generator (an LLM to craft an answer)
          </p>

          <aside class="notes">
            Retrieval-Augmented Generation couples a retriever with a generator
            so the LLM is grounded in real data. Think of it as 'search then
            answer' rather than 'just answer'. We'll build both halves today.
          </aside>
        </section>

        <!-- ===== WHAT IS ICD-11 ===== -->
        <section>
          <h2>
            The Knowledge Base: <span class="gradient-title">ICD-11</span>
          </h2>

          <p>
            The International Classification of Diseases, 11th Revision (ICD-11)
            is the global standard for diagnostic health information
          </p>
          <p>
            The ICD-11 now catalogues roughly
            <span id="counting-number" class="highlight-number">0</span>
            distinct clinical concepts, giving health professionals an
            unprecedented breadth of diagnostic detail
          </p>
          <p>
            You can learn more at the
            <a href="https://icd.who.int/en" target="_blank"
              >official WHO site</a
            >
          </p>

          <aside class="notes">
            Before we write a single line of code, let's examine what we're
            indexing. ICD-11 is the World Health Organization's global standard
            for diagnostic codes. It went live in January 2022 and now contains
            ≈17000 entities and ≈120000 index terms.
          </aside>
        </section>

        <!-- ===== ICD-11 DATA USAGE ===== -->
        <section>
          <h2>
            The Knowledge Base: <span class="gradient-title">ICD-11</span>
          </h2>
          <p>
            Our RAG system will use this data from Chapter 6 to obtain
            suggestions for diagnoses
          </p>

          <aside class="notes">
            Chapter 06 - Mental, behavioural or neurodevelopmental disorders.
          </aside>
        </section>

        <!-- ===== ICD-11 BROWSER ===== -->
        <section
          data-background-color="white"
          data-background-iframe="#"
          data-background-interactive
        >
          <aside class="notes">
            Each entity ships with: an alphanumeric code (e.g., 6B60), a
            preferred title and synonyms, a concise definition, hierarchical
            parents/children and explicit exclusions. For this lab we consider
            That subset has 557 entities—big enough to stress-test retrieval.
          </aside>
        </section>

        <!-- ===== PART 2: PROJECT SETUP ===== -->
        <section>
          <h2 class="title-animate">
            <span class="medium-slate-blue">Part 2</span>: Project Setup
          </h2>

          <aside class="notes">
            Let's get our environment ready. We'll start with the prerequisites
            and then move on to the setup.
          </aside>
        </section>

        <!-- ===== LOCAL ENVIRONMENT VS. GOOGLE COLAB ===== -->
        <section>
          <h2>Local Environment vs. Google Colab</h2>
          <p>You can run Python code in different places:</p>
          <div style="display: flex; gap: 20px">
            <div
              class="fragment"
              style="
                flex: 1;
                background: rgba(255, 255, 255, 0.1);
                padding: 15px;
                border-radius: 10px;
              "
            >
              <h4>Your PC</h4>
              <ul>
                <li>
                  <strong>Setup:</strong> You install Python and libraries
                  yourself
                </li>
                <li><strong>Control:</strong> You have full control</li>
                <li><strong>Offline:</strong> Works without internet</li>
              </ul>
            </div>
            <div
              class="fragment"
              style="
                flex: 1;
                background: rgba(255, 255, 255, 0.1);
                padding: 15px;
                border-radius: 10px;
              "
            >
              <h4>
                Google Colab
                <img
                  class="inline-logo"
                  src="imgs/logo/colab-logo.png"
                  alt="Colab Logo"
                />
              </h4>
              <ul>
                <li><strong>Setup:</strong> No setup needed!</li>
                <li>
                  <strong>Pre-installed:</strong> Many libraries are already
                  available
                </li>
                <li>
                  <strong>Hardware:</strong> Free access to powerful hardware
                </li>
              </ul>
            </div>
          </div>
        </section>

        <section>
          <h2>Google Colab</h2>
          <p>TODO DAVIDE: link a colab senza codice?</p>
        </section>

        <!-- ===== PREREQUISITES ===== -->
        <section>
          <h2>2.1 Prerequisites</h2>
          <p>To follow along, you will need:</p>
          <ul>
            <li>
              <strong>Google API Key:</strong> get one from
              <a href="https://aistudio.google.com/app/apikey" target="_blank"
                >Google AI Studio</a
              >
              <img
                class="inline-logo"
                src="imgs/logo/aistudio-logo.png"
                alt="AI Studio Logo"
              />
            </li>
            <li>
              <strong>ICD-11 Chapter 6 Dataset:</strong> get from
              <a
                href="https://drive.google.com/file/d/1ThIsNf1iuns9wlMZmBHOWRI9E6FiVgjQ/view?usp=drive_link"
                target="_blank"
                >Drive</a
              >
              <img
                class="inline-logo"
                src="imgs/logo/drive-logo.png"
                alt="Drive Logo"
              />
            </li>
          </ul>

          <aside class="notes">
            You'll need two things: a Google API key (grab it from AI Studio)
            and the Chapter 6 CSV which we've already placed in the Google
            Drive.
          </aside>
        </section>

        <section data-background-video="https://www.youtube.com/watch?v=T1BTyo1A4Ww" data-background-video-loop data-background-video-muted>
          <h2>
            Google API Key
            <img
              class="inline-logo"
              src="imgs/logo/aistudio-logo.png"
              alt="AI Studio Logo"
            />
          </h2>
        </section>

        <section>
          <h2>ICD-11 Chapter 6 Dataset</h2>
          <img src="imgs/screenshot.png"/>
        </section>
        <section>
					<h2>ICD-11 Chapter 6 dataset</h2>
					<ul>
						<li class="fragment">Total Entries: <strong>720</strong></li>
						<li class="fragment">Most Frequent Category: <strong>6A60</strong> (appearing 18 times)</li>
						<li class="fragment">Average Title Length: <strong>55 characters</strong></li>
						<li class="fragment">Average Definition Length: <strong>660 characters</strong></li>
					</ul>
				</section>

                <!-- Slide 3: Category Distribution Chart -->
                <section>
                    <h2>Top 20 Category Codes</h2>
                    <!-- 
                        IMPORTANT: Please replace the 'src' attribute below with the path 
                        to the 'category_distribution.png' file that was generated.
                        For example, if it's in the same folder, you can just use "category_distribution.png".
                    -->
                    <img src="imgs/scheme/codes_distribution.png" 
                         alt="Top 20 Category Codes Distribution"
                         style="max-height: 500px; width: auto;">
                    <p class="notes">This chart shows the most common parent-level categories in the dataset.</p>
                </section>

                <!-- Slide 4: Column Definitions -->
                <section>
                    <h2>Column Definitions</h2>
                    <dl>
                        <dt class="fragment">code</dt>
                        <dd class="fragment">The unique alphanumeric identifier for a specific disease or disorder.</dd>
                        <dt class="fragment">title</dt>
                        <dd class="fragment">The official, human-readable name of the condition.</dd>
                        <dt class="fragment">definition</dt>
                        <dd class="fragment">A detailed clinical description of the disorder.</dd>
                    </dl>
                </section>
                <section>
                    <h2>Column Definitions</h2>
                    <dl>
                        <dt class="fragment">inclusions</dt>
                        <dd class="fragment">Other conditions or synonyms that are included within this diagnostic category.</dd>
                        <dt class="fragment">exclusions</dt>
                        <dd class="fragment">Conditions that are explicitly NOT diagnosed under this code.</dd>
                    </dl>
                </section>
                <section>
                    <h2>Column Definitions</h2>
                    <dl>
                        <dt class="fragment">diagnosticCriteria</dt>
                        <dd class="fragment">The specific criteria that must be met for a definitive diagnosis.</dd>
                        <dt class="fragment">category_code</dt>
                        <dd class="fragment">A broader, parent-level category that groups together related codes.</dd>
                    </dl>
                </section>

        <!-- ===== SETUP ===== -->
        <section>
          <h2>Set up</h2>
          <p>
            We begin by setting up a
            <img
              class="inline-logo"
              src="imgs/logo/python-logo.png"
              alt="Python Logo"
            />
            environment. This involves installing all the necessary libraries
            that form the building blocks of our application
          </p>
        </section>

        <section>
          <h2>ENV</h2>
        </section>

        <!-- ===== OUR TOOLS: THE LIBRARIES ===== -->
        <section>
          <h2>Our Tools: The Libraries</h2>
          <p>
            Python's power comes from its vast collection of
            libraries—pre-packaged code that we can use. Here are the key ones
            for our project:
          </p>
          <ul>
            <li>
              LangChain
              <img
                class="inline-logo"
                src="imgs/logo/langchain-logo.png"
                alt="LangChain Logo"
              />: The main framework for creating applications with LLMs
            </li>
          </ul>

          <aside class="notes">
            LangChain is an open-source framework designed to simplify the
            development of applications that use large language models (LLMs).
            It allows developers to build powerful and flexible systems that
            combine LLMs with external data sources, APIs, and custom logic. At
            its core, LangChain helps manage the flow of information between the
            model and the environment. It provides modular components for key
            functions such as prompt management, memory, chaining multiple
            operations, and integration with tools like search engines,
            databases, or user-defined functions. One of LangChain's main
            strengths is its ability to "chain" together different steps of
            reasoning or data processing—hence the name. For example, an
            application might retrieve relevant documents, summarise them, and
            then answer a user's question using the summarised content—all
            orchestrated through LangChain. LangChain is widely used in
            real-world applications such as intelligent agents, chatbots,
            research assistants, and knowledge-driven interfaces. It supports
            multiple programming environments, with Python and JavaScript being
            the most popular. By abstracting away common challenges in working
            with LLMs, LangChain makes it easier to prototype and deploy
            advanced AI applications quickly and reliably.
          </aside>
        </section>

        <!-- ===== OUR TOOLS: THE LIBRARIES ===== -->
        <section>
          <h2>Our Tools: The Libraries</h2>
          <ul>
            <li>
              <strong
                >Pandas
                <img
                  class="inline-logo"
                  src="imgs/logo/pandas-logo.png"
                  alt=""
                />:</strong
              >
              A powerful tool for reading and manipulating data, like our CSV
              file
            </li>
            <li class="fragment">
              <strong
                >Chroma
                <img
                  class="inline-logo"
                  src="imgs/logo/chroma-logo.png"
                  alt=""
                />:</strong
              >
              A special database (vector store) for storing and searching our
              text data efficiently
            </li>
            <li class="fragment">
              <strong
                >Google GenAI
                <img
                  class="inline-logo"
                  src="imgs/logo/google-icon.png"
                  alt=""
                />:</strong
              >
              The library that lets us connect to and use Google's powerful AI
              models
            </li>
          </ul>

          <aside class="notes">
            Pandas is a fast, powerful, and flexible open-source data analysis
            and manipulation library for Python. It provides easy-to-use data
            structures, such as DataFrame and Series, which allow users to
            clean, transform, analyse, and visualise structured data
            efficiently. pandas is widely used in data science, machine
            learning, and statistical analysis thanks to its intuitive syntax
            and strong integration with other Python libraries. Chroma is a
            vector database that allows you to store and search for text data
            efficiently. It is designed to be used with large language models
            (LLMs) and other AI applications that need to process and retrieve
            text data. Google GenAI is a library that allows you to connect to
            and use Google's powerful AI models. It provides a simple interface
            for interacting with these models, making it easy to build and
            deploy AI applications.
          </aside>
        </section>

        <section>
          <h2>
            What is an <code><span class="badge">import</span></code> Statement?
          </h2>
          <p>
            Think of libraries as toolboxes. An
            <code><span class="badge">import</span></code> statement is how we
            tell Python to open a toolbox and take out a specific tool to use
          </p>
          <pre class="fragment">
                      <code data-trim data-line-numbers="1-2|4-5">
                        # This line says: "From the langchain toolbox, get me the 'Document' tool."
                        from langchain_core.documents import Document
      
                        # Now we can use the 'Document' tool in our code.
                        my_doc = Document(page_content="This is a sample document.")
                    </code>
                  </pre>
          <p class="fragment">
            Without importing, Python wouldn't know what a Document is
          </p>
        </section>

        <!-- ===== INSTALLING LIBRARIES ===== -->
        <section>
          <h2>Local Environment vs. Google Colab</h2>
          <p>For this LAB, both work perfectly fine!</p>
          <p>
            Colab
            <img
              class="inline-logo"
              src="imgs/logo/colab-logo.png"
              alt="Colab Logo"
            />
            is often easier to start with
          </p>
        </section>

        <!-- Slide: Setup - Step 1.1: Installing Libraries -->
        <section>
          <h2>Setup - Step 1.1: Installing Libraries</h2>
          <p>
            We use <code><span class="badge">>pip</span></code> to install
            LangChain and its related packages, including the integration for
            Google's AI models and the Chroma
            <img class="inline-logo" src="imgs/logo/chroma-logo.png" alt="" />
            vector store
          </p>
          <pre>
				<code data-line-numbers="1|2|3|4"  data-trim data-noescape>
          pip install -qU python-dotenv requests pandas
          pip install -qU langchain langchain-community
          pip install -qU langchain-google-genai
          pip install -qU langchain-chroma
				</code>
			</pre>
        </section>

        <section>
          <h2>Theory: Securing Credentials</h2>
          <p>
            Hard-coding sensitive information like API keys directly in the
            source code is a major security risk. A best practice is to load
            them from environment variables, which keeps them separate from the
            code
          </p>
          <p class="fragment">
            The <code><span class="badge">python-dotenv</span></code> library
            helps manage this by loading variables from a
            <code><span class="badge">.env</span></code> file for local
            development
          </p>
        </section>
        <section>
          <h2>First Code: Configuring the API Key</h2>
          <p>
            It's time to write our first lines of Python. The most crucial first
            step is to authenticate our script with Google's AI services. This
            code block handles that by securely loading our API key, which is
            like a password for our program
          </p>
          <pre>
            <code data-line-numbers="1-3|5-6|8-10" data-trim data-noescape>
              import os
              import getpass
              from dotenv import load_dotenv
              
              # Load environment variables from .env file (optional)
              load_dotenv()
              
              # Get Google API Key
              if "GOOGLE_API_KEY" not in os.environ:
                os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API Key: ")
            </code>
          </pre>
        </section>
        <section>
          <h2>Theory: Loading External Data</h2>
          <p class="fragment">
            The first step in the
            <span class="gradient-title">RAG</span> process is "Retrieval". This
            requires loading our external knowledge into a format the
            application can use. LangChain provides
            <code><span class="badge">Document</span></code> objects for this,
            which contain text
            <code><span class="badge">page_content</span></code> and associated
            metadata
          </p>
          <p class="fragment">
            We will write a custom function using
            <code><span class="badge">pandas</span></code> to read our specific
            CSV file and transform each row into a
            <code><span class="badge">LangChain Document</span></code>
          </p>
        </section>
        <section>
          <h2>Theory: Why Split Text for Retrieval?</h2>
          <p class="fragment">
            Once raw documents are loaded, they often need to be broken down
            into smaller, manageable chunks. This is crucial for effective
            retrieval because:
          </p>
          <p class="fragment">
            The goal is to split text intelligently, maintaining semantic
            coherence within each chunk while ensuring no crucial information is
            cut off mid-sentence or mid-paragraph
          </p>
        </section>
        <section>
          <h2>Theory: The Goal of Splitting</h2>
          <ul>
            <li class="fragment">
              <strong>Relevance:</strong> LLMs have context windows. Sending too
              much irrelevant information can dilute the useful context. Smaller
              chunks mean the retriever finds more
              <strong>precise</strong> relevant passages
            </li>
            <li class="fragment">
              <strong>Efficiency:</strong> Embedding and retrieving smaller
              chunks is faster and uses less memory
            </li>
            <li class="fragment">
              <strong>Granularity:</strong> Ensures that retrieved information
              is at the right level of detail for the LLM to synthesize
            </li>
          </ul>
        </section>
        <section>
          <h2>Our Data Format: The CSV File</h2>
          <p>
            Our ICD-11 knowledge is stored in a
            <strong>CSV (Comma-Separated Values)</strong> file. It's one of the
            simplest and most common ways to store tabular data
          </p>
          <p class="fragment">
            Think of it as a plain text spreadsheet. Each line is a row, and
            commas separate the columns. Because it's so simple, it's incredibly
            easy for almost any programming language, including Python, to read
            and work with
          </p>
        </section>
        <section>
          <h2>Step 1.3: Implementing the CSV Loader</h2>
          <p>
            The function in the next slide reads the
            <code><span class="badge">icdchapter6.csv</span></code> file,
            iterates through each row, and creates a
            <code><span class="badge">Document</span></code> object with
            formatted content and metadata
          </p>
        </section>

        <section>
          <h2>Step 1.3: Implementing the CSV Loader</h2>
          <div class="content">
            <p class="description">
              The function in the next slide reads the
              <code><span class="badge">icdchapter6.csv</span></code> file,
              iterates through each row, and creates a
              <code><span class="badge">Document</span></code> object with
              formatted content and metadata
            </p>
            <div class="code-block">
              <pre><code data-line-numbers data-trim data-noescape>
                import pandas as pd
                from langchain_core.documents import Document

                def load_icd11_from_csv(file_path: str) -> list[Document]:
                      df = pd.read_csv(file_path, delimiter=';')
                      icd_documents = []
                      for index, row in df.iterrows():
                        content = (f"ICD-11 Code: {row['code']}\\n"
                              f"Title: {row['title']}\\n"
                              f"Definition: {row['definition']}")
                        metadata = {"code": row['code'], "title": row['title']}
                        doc = Document(page_content=content, metadata=metadata)
                        icd_documents.append(doc)
                      return icd_documents
                    
                # Let's load the documents
                icd11_documents = load_icd11_from_csv('icdchapter6.csv')
                print(f"Loaded {len(icd11_documents)} documents.")
              </code></pre>
            </div>
          </div>
        </section>

        <!-- ===== WHY THESE DATA FIELDS? STRUCTURING THE CONTEXT ===== -->
        <section>
          <h2>Why These Data Fields? Structuring the Context</h2>
          <p>
            In our loader, we combined the <strong>code</strong>,
            <strong>title</strong>, and <strong>definition</strong> for a reason
          </p>
          <ul class="fragment">
            <li>
              <strong>Code & Title:</strong> These provide clear, structured
              identifiers. They are perfect for when the LLM needs to give a
              precise, factual answer
            </li>
            <li>
              <strong>Definition:</strong> This is the most important part for
              our retriever. It contains the rich, descriptive text that allows
              for effective <strong>semantic search</strong>, helping the system
              find relevant concepts even if the query doesn't use exact
              keywords
            </li>
          </ul>
        </section>

        <section>
          <h2>Why These Data Fields? Structuring the Context</h2>
          <p>
            By combining them, we create a comprehensive
            <code><span class="badge">Document</span></code> that gives the LLM
            everything it needs: a unique ID, a clear label, and a detailed
            description to inform its answer
          </p>
        </section>
        <section>
          <h2 class="r-fit-text checkpoint-animate">Lab Checkpoint 1</h2>
          <p>
            Your script should now handle installation, API keys, and data
            loading.
          </p>
          <pre>
            <code data-line-numbers="1-4|6-9|11-21|23-29" data-trim data-noescape>
              import os, getpass
              from dotenv import load_dotenv
              import pandas as pd
              from langchain_core.documents import Document
              
              # API KEY SETUP
              load_dotenv()
              if "GOOGLE_API_KEY" not in os.environ:
                os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter Google API Key: ")
              
              # DATA LOADING FUNCTION
              def load_icd11_from_csv(file_path: str) -> list[Document]:
                try:
                  df = pd.read_csv(file_path, delimiter=';'); docs = []
                  for i, row in df.iterrows():
                    c = (f"ICD-11 Code: {row['code']}\\n"
                      f"Title: {row['title']}\\n"
                      f"Definition: {row['definition']}")
                    docs.append(Document(page_content=c, metadata={"code": row['code'], "title": row['title']}))
                  return docs
                except FileNotFoundError: return []
              
              # EXECUTE LOADING
              icd11_documents = load_icd11_from_csv('icdchapter6.csv')
              if icd11_documents:
                print(f"Loaded {len(icd11_documents)} documents.")
                icd11_chunks = icd11_documents
              else:
                print("Failed to load documents.")
						  </code>
            </pre>
        </section>

        <!-- ===== TITLE SLIDE ===== -->
        <section>
          <h2 class="title-animate">
            <span class="selective-yellow">Part 3</span>: Embeddings & Vector
            Stores
          </h2>
        </section>

        <!-- ===== EMBEDDINGS & VECTOR STORES ===== -->
        <section>
          <h2>Theory: Embeddings & Vector Stores</h2>
          <p class="fragment">
            Now that we have our data loaded, we need to make it searchable.
          </p>
          <p class="fragment">
            We'll do this by converting our text into numerical representations
            and storing them in a specialized database
          </p>
        </section>

        <!-- ===== THEORY: CREATING TEXT EMBEDDINGS ===== -->
        <section>
          <h2>Theory: Creating Text Embeddings</h2>
          <div class="content">
            <div class="theory-block">
              <p class="main-point">
                To find relevant information, we can't just match keywords. We
                need to understand the <strong>meaning</strong> or
                <strong>semantic content</strong> of the text.
              </p>
              <p class="fragment detail-point">
                <strong>Embeddings</strong> are numerical vectors that represent
                this semantic meaning. An embedding model converts our text
                documents into these vectors, placing similar concepts close to
                each other in vector space.
              </p>
            </div>
          </div>
        </section>
        <section>
          <h2>A Note on Debugging</h2>
          <p>
            Errors are a normal part of coding! <strong>Debugging</strong> is
            the process of finding and fixing them. No one writes perfect code
            on the first try.
          </p>
          <p class="fragment">
            The simplest yet most powerful tool in your debugging toolkit is the
            <code><span class="badge">print()</span></code> statement. If you're
            not sure what a variable holds or if a line of code is being
            reached, just print it! In Colab, the output will appear right below
            the code cell.
          </p>
        </section>

        <!-- ===== A NOTE ON DEBUGGING ===== -->
        <section>
          <h2>A Note on Debugging</h2>
          <p>You can print text, variables or both. Here are some examples:</p>
          <pre class="fragment">
          <code data-line-numbers="1|2|3" data-trim data-noescape>
            print("just a literal text")
            print(variable_name)
            print(f"String interpolation: {variable_name}")
          </code>
        </pre>
        </section>

        <!-- ===== STEP 2.1: INITIALIZING THE EMBEDDING MODEL ===== -->
        <section>
          <h2>Step 2.1: Initializing the Embedding Model</h2>
          <p>
            We'll use Google's
            <code><span class="badge">embedding-001</span></code> model via the
            <code><span class="badge">GoogleGenerativeAIEmbeddings</span></code>
            class in LangChain to perform this conversion
          </p>
          <pre>
            <code data-line-numbers data-trim data-noescape>
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
		
            # Initialize the embedding model
            embeddings = GoogleGenerativeAIEmbeddings(
              model="models/embedding-001"
            )
            print("Embedding model initialized.")
            </code>
          </pre>
        </section>

        <!-- ===== THEORY: INDEXING IN A VECTOR STORE ===== -->
        <section>
          <h2>Theory: Indexing in a Vector Store</h2>
          <p>
            Searching through thousands of embeddings one-by-one would be very
            slow. A <strong>Vector Store</strong> is a specialized database
            designed to store and efficiently search these high-dimensional
            vectors using fast algorithms like Approximate Nearest Neighbor
            (ANN) search
          </p>
          <p class="fragment">
            We will use
            <strong>Chroma</strong>
            <img class="inline-logo" src="imgs/logo/chroma-logo.png" alt="" />,
            a popular open-source vector store that runs in memory but can be
            persisted to disk to avoid re-processing
          </p>
        </section>

        <!-- ===== IMPORTANT NOTES ON VECTOR STORES ===== -->
        <section>
          <h2>Important notes on vector stores</h2>
          <p class="fragment">
            The embedding process requires lot of computation power, and we do
            not want to repeat each time
          </p>
          <p class="fragment">
            For this example we are going to store on a local disk the
            vectorstore, so we can retrieve multiples times without reprocessing
            the embeddings
          </p>
        </section>

        <!-- ===== STEP 2.2: CREATING THE VECTOR STORE ===== -->
        <section>
          <h2>Step 2.2: Creating the Vector Store</h2>
          <p>
            This code creates a new Chroma database from our documents and
            embeddings if one doesn't already exist, or loads the existing one
            from disk
          </p>
          <pre>
            <code data-line-numbers="1-2|4|6-11|12-17|19-21" data-trim data-noescape>
              from langchain_chroma import Chroma
              import os
                  
              persist_directory = "./chroma_db_workshop"
                  
              # Load from disk if it exists, otherwise create it
              if os.path.exists(persist_directory):
                vectorstore = Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
                )
              else:
                vectorstore = Chroma.from_documents(
                      documents=icd11_chunks,
                      embedding=embeddings,
                      persist_directory=persist_directory
                    )
                  
              # The retriever is our interface for searching
              retriever = vectorstore.as_retriever()
              print("Retriever is ready.")
								</code>
              </pre>
        </section>

        <!-- ===== LAB CHECKPOINT 2 ===== -->
        <section>
          <h2 class="r-fit-text checkpoint-animate">LAB Checkpoint 2</h2>
          <p>
            Your script should now include the embedding model and vector store
            creation.
          </p>
          <pre>
            <code data-line-numbers data-trim data-noescape>
                # ... (previous code from Checkpoint 1) ...
                from langchain_google_genai import GoogleGenerativeAIEmbeddings
                from langchain_chroma import Chroma
                
                # ... (API Key and Data Loading code) ...
                icd11_chunks = load_icd11_from_csv('icdchapter6.csv')
                print(f"Loaded {len(icd11_chunks)} chunks.")
                
                # EMBEDDING MODEL
                embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
                
                # VECTOR STORE
                persist_directory = "./chroma_db_workshop"
                if os.path.exists(persist_directory) and os.listdir(persist_directory):
                  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
                  print("Loaded existing vector store.")
                else:
                  vectorstore = Chroma.from_documents(documents=icd11_chunks, embedding=embeddings, persist_directory=persist_directory)
                  print("Created new vector store.")
                
                retriever = vectorstore.as_retriever()
                print("Retriever is ready.")
								 </code></pre>
        </section>

        <section>
          <h2 class="title-animate">
            <span class="tangerine">Part 4</span>: Building & Querying the RAG
            Chain
          </h2>
        </section>

        <!-- ===== RAG ===== -->
        <section>
          <h2>Building & Querying the RAG Chain</h2>
          <p>
            Now we have all the components: a retriever to fetch data and an LLM
            to generate answers. The final step is to orchestrate them into a
            single, cohesive application
          </p>
        </section>

        <section>
          <h2>Theory: Composing with LCEL</h2>
          <p>
            <strong>LangChain Expression Language (LCEL)</strong> is a
            declarative way to compose components into chains. The pipe (`|`)
            operator connects each step, passing the output of one step as the
            input to the next
          </p>
          <div class="fragment">
            <p>Our chain will be:</p>
            <ol>
              <li>The user's question retrieves context</li>
              <li>The question and context populate a prompt</li>
              <li>The prompt is sent to the LLM</li>
              <li>The LLM's response is parsed into a clean string</li>
            </ol>
          </div>
        </section>

        <!-- ===== THEORY: THE ART OF THE PROMPT ===== -->
        <section>
          <h2>The Art of the Prompt</h2>
          <p>
            A <strong>prompt</strong> is the set of instructions we give to the
            LLM. It's the most direct way we have to control its behavior
          </p>
          <p class="fragment">
            <strong>Prompt Engineering</strong> is the skill of crafting these
            instructions to get the exact output you want. A good prompt is
            clear, specific, and provides all the necessary context for the
            model to succeed
          </p>
        </section>

        <!-- ===== THEORY: HOW RAG USES A PROMPT TEMPLATE ===== -->
        <section>
          <h2>
            How <span class="gradient-title">RAG</span> Uses a Prompt Template
          </h2>
          <p>
            We don't write a new prompt for every user question. Instead, we use
            a <strong>template</strong>. LangChain dynamically inserts the
            retrieved context and the user's original question into the
            placeholders of this template
          </p>
          <pre class="fragment">
            <code data-line-numbers data-trim data-noescape>
              # Our template has two placeholders: {context} and {question}
              template = """Use the retrieved context to answer the question.

              Context: {context}
              Question: {question}"""

              # LangChain will automatically fill these in before sending to the LLM
            </code>
            </pre>
        </section>

        <!-- ===== GIVING CLEAR INSTRUCTIONS ===== -->
        <section>
          <h2>Giving Clear Instructions</h2>
          <p>
            The most important part of our prompt is the instructions that guide
            the LLM's behavior. Consider this line:
          </p>
          <p
            class="fragment highlight-red"
            style="text-align: center; margin: 20px 0; font-style: italic"
          >
            "If you don't know the answer, just say that you don't know."
          </p>
          <p class="fragment">
            This single instruction is critical for building a trustworthy
            system. It explicitly tells the LLM not to guess or "hallucinate" an
            answer if the information isn't in the retrieved context. This makes
            our RAG system more reliable and factual
          </p>
        </section>

        <!-- ===== STEP 3.1: CONSTRUCTING THE CHAIN ===== -->
        <section>
          <h2>Step 3.1: Constructing the Chain</h2>
          <p>
            We define a prompt template and then pipe together the retriever,
            the prompt, the LLM, and an output parser.
          </p>
          <pre>
              <code data-line-numbers data-trim data-noescape>
                  from langchain_google_genai import ChatGoogleGenerativeAI
                  from langchain_core.prompts import ChatPromptTemplate
                  from langchain_core.output_parsers import StrOutputParser
                  from langchain_core.runnables import RunnablePassthrough
                  
                  # Initialize the Language Model (LLM)
                  llm = ChatGoogleGenerativeAI(model="gemma-3-1b-it", temperature=0.7)
                  
                  # Define the prompt template
                  template = """You are a helpful assistant for ICD-11.
                  Use the retrieved context to answer the question.
                  If you don't know the answer, just say that you don't know.
                  
                  Context: {context}
                  Question: {question}
                  Answer:"""
                  prompt = ChatPromptTemplate.from_template(template)
                  
                  # Construct the RAG chain with LCEL
                  rag_chain = (
                    {"context": retriever, "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
                  )
                  
                  print("RAG chain constructed.")
								</code>
              </pre>
        </section>

        <section>
          <h2 class="title-animate">
            <span class="persimmon">Part 5</span>: Querying the RAG System
          </h2>
        </section>

        <!-- ===== THEORY: QUERYING THE SYSTEM ===== -->
        <section>
          <h2>Querying the System</h2>
          <p>
            The <code><span class="badge">rag_chain</span></code> we created is
            now a runnable object. We can use its
            <code><span class="badge">invoke()</span></code> method to pass in a
            user query. This triggers the entire sequence of operations we
            defined, returning the final, context-aware answer from the LLM
          </p>
        </section>

        <!-- ===== STEP 3.2: PUTTING IT TO THE TEST ===== -->
        <section>
          <h3>Step 3.2: Putting it to the Test</h3>
          <p>
            We define a helper function and then ask our RAG system a question
            that is inside its knowledge base and one that is outside of it.
          </p>
          <pre>
              <code data-line-numbers data-trim data-noescape>
                  def ask_rag_system(query: str):
                    print(f"\\n--- Asking: '{query}' ---")
                    response = rag_chain.invoke(query)
                    print(f"Answer:\\n{response}")
                  
                  # Query 1: Specific question
                  ask_rag_system("What is the ICD-11 code for 'Dissociative identity disorder'?")
                  
                  # Query 2: Off-topic question
                  ask_rag_system("What is the capital of France?")
								</code>
              </pre>
        </section>

        <section>
          <h2 class="r-fit-text">Final Code: Complete RAG System</h2>
          <p>
            This is the entire script for our RAG system, from start to finish.
          </p>
          <pre>
              <code data-line-numbers data-trim data-noescape>
                  import os, getpass
                  from dotenv import load_dotenv
                  import pandas as pd
                  from langchain_core.documents import Document
                  from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
                  from langchain_chroma import Chroma
                  from langchain_core.prompts import ChatPromptTemplate
                  from langchain_core.output_parsers import StrOutputParser
                  from langchain_core.runnables import RunnablePassthrough
                  
                  # Setup
                  load_dotenv()
                  if "GOOGLE_API_KEY" not in os.environ:
                    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter Google API Key: ")
                  
                  # Data Loading ... (condensed function)
                  def load_icd11_from_csv(file_path: str): 
                    try:
                      df=pd.read_csv(file_path,delimiter=';');docs=[]
                      for i, r in df.iterrows():
                        c = f"Code: {r['code']}\\nTitle: {r['title']}\\nDef: {r['definition']}"
                        docs.append(Document(page_content=c, metadata={"code": r['code']}))
                      return docs
                    except FileNotFoundError: return []
                  icd11_chunks = load_icd11_from_csv('icdchapter6.csv')
                  
                  # Embeddings & Vector Store
                  embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
                  vectorstore = Chroma(persist_directory="./chroma_db_workshop", embedding_function=embeddings)
                  retriever = vectorstore.as_retriever()
                  
                  # RAG Chain
                  llm = ChatGoogleGenerativeAI(model="gemma-3-1b-it", temperature=0.7)
                  template = "Context: {context}\\nQuestion: {question}\\nAnswer:"
                  prompt = ChatPromptTemplate.from_template(template)
                  rag_chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())
                  
                  # Querying
                  def ask_rag_system(query: str):
                    response = rag_chain.invoke(query)
                    print(f"\\nQuery: {query}\\nAnswer: {response}")
                  
                  ask_rag_system("What is 'Dissociative identity disorder'?")
                  ask_rag_system("What is the capital of France?")
								</code>
              </pre>
        </section>

        <!-- ===== WORKSHOP WRAP-UP ===== -->
        <section>
          <div>
            <ul class="content-list">
              <li class="list-item">
                <span class="text">Set up a Python environment...</span>
              </li>
              <li class="list-item">
                <span class="text">Loaded external knowledge...</span>
                <code><span class="badge">Documents</span></code>
              </li>
              <li class="list-item">
                <span class="text">Generated semantic embeddings...</span>
              </li>
              <li class="list-item">
                <span class="text">Indexed these embeddings...</span>
              </li>
              <li class="list-item">
                <span class="text">Constructed a complete RAG chain...</span>
              </li>
              <li class="list-item">
                <span class="text">Queried our system...</span>
              </li>
            </ul>
          </div>
        </section>

        <section>
          <h2>Beyond Basics: Limitations & Next Steps</h2>
          <div class="content">
            <p>
              While powerful, RAG systems are not without their challenges.
              Understanding their limitations is key to building more robust and
              reliable applications.
            </p>
          </div>
        </section>

        <section>
          <h2>Limitations of Basic RAG</h2>
          <div class="content">
            <p>
              Our current RAG system is a solid foundation, but basic RAG can
              face issues:
            </p>
          </div>
          <ul>
            <li class="fragment">
              <strong>Context Window Issues:</strong> Even with splitting, very
              long or complex retrieved contexts might still exceed LLM limits
              or dilute relevance.
            </li>
            <li class="fragment">
              <strong>"Lost in the Middle":</strong> LLMs sometimes pay less
              attention to information in the middle of a long context.
            </li>
            <li class="fragment">
              <strong>Retrieval Quality:</strong> If the retriever doesn't find
              the *most* relevant chunks, the LLM's answer will suffer. Semantic
              search isn't perfect.
            </li>
            <li class="fragment">
              <strong>Hallucinations Persist:</strong> While reduced, LLMs can
              still hallucinate if the retrieved context is ambiguous or
              insufficient.
            </li>
            <li class="fragment">
              <strong>Data Freshness:</strong> The RAG system is only as current
              as its knowledge base. Updating it requires re-indexing.
            </li>
          </ul>
        </section>

        <section>
          <h2>Advanced RAG Techniques</h2>
          <div class="content">
            <p>To address these limitations, advanced RAG patterns exist:</p>
          </div>
          <ul>
            <li class="fragment">
              <strong>Query Transformation:</strong> Rewriting or expanding user
              queries (e.g., adding keywords, generating multiple sub-queries)
              to improve retrieval.
            </li>
            <li class="fragment">
              <strong>Re-ranking:</strong> After initial retrieval, use a
              separate re-ranking model to score and reorder documents based on
              relevance, ensuring the most pertinent ones are at the top.
            </li>
            <li class="fragment">
              <strong>Hybird Search:</strong> Combining semantic search with
              traditional keyword search (e.g., BM25) for more comprehensive
              retrieval.
            </li>
            <li class="fragment">
              <strong>Multi-stage Retrieval:</strong> Using multiple retrieval
              steps, perhaps first to identify broad topics, then specific
              details.
            </li>
            <li class="fragment">
              <strong>Agentic RAG:</strong> Using an LLM to decide *how* to
              retrieve information (e.g., searching different tools or
              databases).
            </li>
          </ul>
        </section>

        <!-- ===== FURTHER READING ===== -->
        <section>
          <h2>For Further Reading</h2>
          <div>
            <p>
              This workshop covered the fundamentals of building a RAG system.
              For a deep, academic dive into the state of the art, advanced
              techniques, and evaluation methodologies, we highly recommend the
              following survey paper:
            </p>
            <ul>
              <li>
                <strong>Title:</strong> A survey on retrieval-augmented
                generation for large language models
              </li>
              <li>
                <strong>Source:</strong>
                <a
                  href="https://www.sciencedirect.com/science/article/pii/S0957417425008139?via%3Dihub"
                  target="_blank"
                  >ScienceDirect</a
                >
              </li>
            </ul>
            <p class="small-text">
              This paper provides a comprehensive overview of the RAG paradigm,
              its various components, and future research directions in the
              field
            </p>
          </div>
        </section>

        <!-- ===== Q&A ===== -->
        <section>
          <h2>Questions?</h2>
        </section>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      Reveal.initialize({
        view: "scroll",
        scrollProgress: true,
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });

      // Reset and replay title animation
      Reveal.on("slidechanged", (event) => {
        const title = event.currentSlide.querySelector(".title-animate");
        if (title) {
          const colorSpan = title.querySelector("span");
          if (colorSpan) {
            const colorClass = colorSpan.className;
            const colorVar = `var(--${colorClass})`;

            const style = document.createElement("style");
            style.textContent = `
              .title-animate::after {
                background: linear-gradient(120deg, ${colorVar} 0%, ${colorVar} 100%) !important;
              }
            `;
            document.head.appendChild(style);
          }

          title.classList.remove("title-animate");
          void title.offsetWidth;
          title.classList.add("title-animate");
        }
      });

      // Add counting animation
      function animateNumber(element, start, end, duration) {
        let startTimestamp = null;
        element.classList.add("active");
        const step = (timestamp) => {
          if (!startTimestamp) startTimestamp = timestamp;
          const progress = Math.min((timestamp - startTimestamp) / duration, 1);
          const current = Math.floor(progress * (end - start) + start);
          element.textContent = current.toLocaleString();
          if (progress < 1) {
            window.requestAnimationFrame(step);
          } else {
            setTimeout(() => {
              element.classList.remove("active");
            }, 500);
          }
        };
        window.requestAnimationFrame(step);
      }

      // Start animation when the slide is shown
      Reveal.on("slidechanged", (event) => {
        if (event.currentSlide.querySelector("#counting-number")) {
          const element = document.getElementById("counting-number");
          animateNumber(element, 0, 85000, 2000);
        }
      });
    </script>
  </body>
</html>
