{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSaFZvKugrxd"
      },
      "source": [
        "# Building a Retrieval Augmented Generation (RAG) System with LangChain and ICD-11 Data\n",
        "\n",
        "This Colab notebook demonstrates how to build a RAG system using LangChain to answer questions based on the International Classification of Diseases, 11th Revision (ICD-11) data.\n",
        "\n",
        "**Objectives:**\n",
        "* Understand the concept of Retrieval Augmented Generation (RAG).\n",
        "* Learn how to acquire and preprocess external knowledge (ICD-11 data).\n",
        "* Utilize LangChain to integrate various components: Document Loaders, Text Splitters, Embedding Models, Vector Stores, and Language Models.\n",
        "* Build and query a simple RAG chain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRAA_6Sfov7a"
      },
      "source": [
        "NOTE: METTERE UNA NUOVA SESEZIONE NUOVA DOVE SI PARLA DI ICD-11, CON IMMAGINI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6sPCNSpC6k"
      },
      "source": [
        "NOTA: QUI I PREREQUISITI CHIAVI, FILE ETC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkwdgTC-p0CZ"
      },
      "source": [
        "NOTA: per ogni dipenza spiegare cosa è, perché viene usato, quali sono le alternative, funzionalità."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vut6SHSHo7P2"
      },
      "source": [
        "## 1. Setup and Installations\n",
        "First, we need to install the necessary Python libraries.\n",
        "\n",
        "\n",
        "# Install LangChain, ChromaDB, and Google Generative AI components\n",
        "!pip install -qU langchain langchain-community langchain-chroma langchain-google-genai python-dotenv requests pandas\n",
        "\n",
        "\n",
        "Next, we'll set up API keys. For this lesson, we will use Google's `gemma-3-1b-it` for the Language Model (LLM) and `GoogleGenerativeAIEmbeddings` for generating text embeddings. You'll need a Google API key.\n",
        "\n",
        "**How to get a Google API Key:**\n",
        "1. Go to [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2. Click \"Get API Key\" or \"Create API Key in new project\".\n",
        "3. Copy the generated API key.\n",
        "\n",
        "**How to get ICD Knowledgebase**\n",
        "1. Go to [Drive](https://drive.google.com/file/d/1ThIsNf1iuns9wlMZmBHOWRI9E6FiVgjQ/view?usp=drive_link)\n",
        "2. Upload here in colab, go to file section, just add it to the root folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "iLHF9KKLfaOJ",
        "outputId": "a18a101d-dce4-431e-dd52-5052ef980a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2823587441>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get Google API Key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your Google API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-community langchain-chroma langchain-google-genai python-dotenv requests pandas\n",
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file if it exists\n",
        "load_dotenv()\n",
        "\n",
        "# Get Google API Key\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFeeDeVZglje"
      },
      "source": [
        "## 2. Acquiring ICD-11 Data from CSV\n",
        "\n",
        "We will load the ICD-11 data directly from the provided `icdchapter6.csv` file. This file contains ICD-11 codes, titles, and definitions, separated by semicolons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXc3fqxXVQ0c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def load_icd11_from_csv(file_path: str) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Loads ICD-11 data from a CSV file and converts it into LangChain Document objects.\n",
        "    Assumes the CSV has columns like 'Code', 'Title', 'Definition'.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: A list of LangChain Document objects.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ICD-11 data from CSV: '{file_path}'...\")\n",
        "    try:\n",
        "        # Read the CSV file using pandas, specifying the delimiter\n",
        "        # Assuming the CSV has headers like 'Code', 'Title', 'Definition'\n",
        "        df = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "        icd_documents = []\n",
        "        for index, row in df.iterrows():\n",
        "            # Adjust column names based on your CSV structure if different\n",
        "            code = row[\"code\"]\n",
        "            title = row[\"title\"]\n",
        "            definition = row[\"definition\"]\n",
        "            #diagnosticCriteria = row[\"diagnosticCriteria\"]\n",
        "            # Create a single string containing all relevant information\n",
        "            content = f\"ICD-11 Code: {code}\\nTitle: {title}\\nDefinition: {definition}\"\n",
        "            # Add metadata for better context if needed during retrieval\n",
        "            metadata = {\"code\": code, \"title\": title,\"definition\":definition}\n",
        "            icd_documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "        print(f\"Successfully loaded {len(icd_documents)} ICD-11 entries from CSV.\")\n",
        "        return icd_documents\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at '{file_path}'. Please ensure the file is uploaded.\")\n",
        "        return []\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing expected column in CSV: {e}. Please check CSV headers.\")\n",
        "        print(\"Expected columns: 'Code', 'Title', 'Definition'\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while reading the CSV: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load ICD-11 documents from the uploaded CSV file\n",
        "# Ensure 'icdchapter6.csv' is uploaded to your Colab environment\n",
        "icd11_documents = load_icd11_from_csv('icdchapter6.csv')\n",
        "\n",
        "if icd11_documents:\n",
        "    print(\"\\nSample ICD-11 Document (first 100 characters):\")\n",
        "    print(icd11_documents[0].page_content[:100], \"...\")\n",
        "    print(\"\\nMetadata example:\")\n",
        "    print(icd11_documents[0].metadata)\n",
        "else:\n",
        "    print(\"Failed to load ICD-11 documents from CSV. Please check the file path and content.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq76vj2MgiFV"
      },
      "source": [
        "## 3. Processing Documents: No Text Splitting\n",
        "\n",
        "As each row in the CSV represents a complete disorder entry and should not be split, we will treat each loaded document as a single chunk. This means we will not apply any further text splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvoglXlFfpXZ"
      },
      "outputs": [],
      "source": [
        "# As requested, each document (representing a row/disorder) will be treated as a single chunk.\n",
        "icd11_chunks = icd11_documents\n",
        "\n",
        "print(f\"\\nOriginal documents count (now also chunk count): {len(icd11_documents)}\")\n",
        "print(f\"Each document is now treated as a single chunk.\")\n",
        "\n",
        "if icd11_chunks:\n",
        "    print(\"\\nSample chunk (first 100 characters):\")\n",
        "    print(icd11_chunks[0].page_content[:100], \"...\")\n",
        "    print(\"\\nSample chunk metadata:\")\n",
        "    print(icd11_chunks[0].metadata)\n",
        "else:\n",
        "    print(\"No chunks available. Please ensure ICD-11 documents were loaded correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_hTrsBAgdiB"
      },
      "source": [
        "## 4. Generating Embeddings\n",
        "\n",
        "Embeddings are numerical representations of text that capture semantic meaning. We'll use these embeddings to find semantically similar chunks when a user asks a question. `GoogleGenerativeAIEmbeddings` will convert our text chunks into vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5Jsaot1fvs5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "# The model name 'models/embedding-001' is suitable for generating text embeddings.\n",
        "# It uses the API key set in the GOOGLE_API_KEY environment variable.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "print(\"Embedding model initialized.\")\n",
        "# You can optionally test an embedding\n",
        "# sample_embedding = embeddings.embed_query(\"What is hypertension?\")\n",
        "# print(f\"Sample embedding dimension: {len(sample_embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvW0H4p6gZiu"
      },
      "source": [
        "\n",
        "## 5. Creating or Loading a Persistent Vector Store\n",
        "\n",
        "A vector store (or vector database) stores the embeddings and allows for efficient similarity search.\n",
        "We'll use `Chroma` and configure it to persist data to disk. This means if you run this notebook again,\n",
        "it will load the existing vector store instead of re-embedding all documents, saving time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geFdyqGUf34e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_chroma import Chroma\n",
        "import os\n",
        "\n",
        "# Define the directory where the vector store will be persisted\n",
        "persist_directory = \"./chroma_db\"\n",
        "\n",
        "# Check if the vector store already exists\n",
        "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "    print(f\"Loading existing Chroma vector store from '{persist_directory}'...\")\n",
        "    # If it exists, load it\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=persist_directory,\n",
        "        embedding_function=embeddings # Correct parameter is embedding_function\n",
        "    )\n",
        "    print(\"Chroma vector store loaded successfully!\")\n",
        "else:\n",
        "    print(\"Chroma vector store not found or is empty. Creating a new one...\")\n",
        "    # If not, create a new one from the documents\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=icd11_chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory # Persist the new vector store\n",
        "    )\n",
        "    print(f\"Chroma vector store created and persisted to '{persist_directory}'!\")\n",
        "\n",
        "# To make the vectorstore a retriever that can be used in the RAG chain:\n",
        "retriever = vectorstore.as_retriever()\n",
        "print(\"Retriever created from the vector store.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0XKQ4BWgRF6"
      },
      "source": [
        "## 6. Building the RAG Chain\n",
        "\n",
        "Now, we'll assemble the RAG chain using LangChain. The chain will perform the following steps:\n",
        "1.  **Retrieve:** Given a user query, use the `retriever` to find the most relevant chunks from our ICD-11 vector store.\n",
        "2.  **Stuff:** Combine these retrieved chunks with the original query into a single prompt for the Language Model.\n",
        "3.  **Generate:** The Language Model generates a response based on the combined prompt.\n",
        "\n",
        "We'll use LangChain Expression Language (LCEL) for a clear and flexible chain construction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBKa3apMf7vA"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialize the Language Model (LLM)\n",
        "# Changed model to 'gemma-3-1b-it' as requested.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemma-3-1b-it\", temperature=0.7)\n",
        "\n",
        "# Define the prompt template for the LLM\n",
        "# The prompt instructs the LLM to act as a helpful assistant and answer questions\n",
        "# based on the provided context, stating if it doesn't know.\n",
        "template = \"\"\"You are a helpful assistant for medical information, specifically regarding ICD-11.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Keep the answer concise and relevant to the provided ICD-11 context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Construct the RAG chain using LCEL\n",
        "# The chain flows from:\n",
        "# 1. 'question' and 'context' inputs. 'context' is populated by the retriever.\n",
        "# 2. These are passed to the prompt.\n",
        "# 3. The prompt is passed to the LLM.\n",
        "# 4. The LLM's output is parsed into a string.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain constructed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KODoGt9PgLJU"
      },
      "source": [
        "## 7. Demonstration & Evaluation\n",
        "\n",
        "Now, let's test our RAG system with some queries related to medical conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd9rXWX7f-_H"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_rag_system(query: str):\n",
        "    \"\"\"\n",
        "    Asks a question to the RAG system and prints the answer.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Asking: '{query}' ---\")\n",
        "    try:\n",
        "        response = rag_chain.invoke(query)\n",
        "        print(f\"Answer:\\n{response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Please ensure your Google API Key is valid and that you have sufficient quota.\")\n",
        "\n",
        "\n",
        "# Example Queries\n",
        "ask_rag_system(\"What is the ICD-11 code and definition for 'Dissociative identity disorder'?\")\n",
        "ask_rag_system(\"What is the capital of France?\") # This query should ideally result in \"I don't know\" as it's outside the ICD-11 context.\n",
        "ask_rag_system(\"What is the ICD-11 code for 'Anxiety'?\") # Test a term that might be less prevalent if max_results was small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSAUnyOmgB_R"
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "You have successfully built a basic RAG system using LangChain, drawing information from ICD-11 data.\n",
        "\n",
        "**Key Learnings:**\n",
        "* How to load and prepare unstructured data for RAG.\n",
        "* The role of text splitting and embeddings.\n",
        "* Creating and using a vector store (Chroma).\n",
        "* Connecting an LLM and retriever to form a RAG chain.\n",
        "\n",
        "**Potential Improvements and Further Exploration:**\n",
        "* **Larger Dataset:** Integrate with the full WHO ICD-11 API (requires authentication and more robust data handling) or download a larger pre-processed dataset if available publicly.\n",
        "* **Advanced Text Splitting:** Experiment with different `TextSplitter` strategies or add metadata to chunks for improved retrieval.\n",
        "* **Hybrid Search:** Combine vector similarity search with keyword-based search for better retrieval performance.\n",
        "* **Reranking:** Implement a reranking step after initial retrieval to ensure the most relevant documents are passed to the LLM.\n",
        "* **Evaluation Metrics:** Set up evaluation metrics to assess the performance of your RAG system (e.g., faithfulness, relevance).\n",
        "* **User Interface:** Build a simple web interface (e.g., using Streamlit or Gradio) to interact with the RAG system.\n",
        "* **Chat History:** Extend the RAG chain to incorporate conversational history for more coherent multi-turn interactions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
